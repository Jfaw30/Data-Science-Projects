{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Streaming application using Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\tWrite code to create a SparkSession, which 1) uses four cores with a proper application name; 2) use the Melbourne timezone; 3) ensure a checkpoint location has been set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType, BooleanType, TimestampType\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.5.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 pyspark-shell'\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"FraudDetectionStreamingApp\")\\\n",
    "    .master(\"local[4]\")\\\n",
    "    .config(\"spark.sql.session.timeZone\", \"Australia/Melbourne\")\\\n",
    "    .config(\"spark.sql.streaming.checkpointLocation\", os.path.join(current_directory, \"temp_checkpoint\"))\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\tSimilar to assignment 2A, write code to define the data schema for the data files, following the data types suggested in the metadata file. Load the static datasets (e.g. customer, product, category) into data frames. (You can use your code from 2A.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (StructType, StructField, StringType, IntegerType, \n",
    "                               DateType, FloatType, TimestampType, BooleanType)\n",
    "\n",
    "# Define schema for customer.csv\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"username\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"birthdate\", DateType(), True),  \n",
    "    StructField(\"first_join_date\", DateType(), True) \n",
    "])\n",
    "\n",
    "# Define schema for category.csv\n",
    "category_schema = StructType([\n",
    "    StructField(\"category_id\", StringType(), True),\n",
    "    StructField(\"cat_level1\", StringType(), True),\n",
    "    StructField(\"cat_level2\", StringType(), True),\n",
    "    StructField(\"cat_level3\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for browsing_behaviour.csv\n",
    "browsing_behaviour_schema = StructType([\n",
    "    StructField(\"session_id\", StringType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"event_time\", TimestampType(), True),  \n",
    "    StructField(\"traffic_source\", StringType(), True),\n",
    "    StructField(\"device_type\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for product.csv\n",
    "product_schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"baseColour\", StringType(), True),\n",
    "    StructField(\"season\", StringType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"usage\", StringType(), True),\n",
    "    StructField(\"productDisplayName\", StringType(), True),\n",
    "    StructField(\"category_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for transaction.csv\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"created_at\", TimestampType(), True),  \n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"session_id\", StringType(), True),\n",
    "    StructField(\"product_metadata\", StringType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"payment_status\", StringType(), True),\n",
    "    StructField(\"promo_amount\", FloatType(), True),\n",
    "    StructField(\"promo_code\", StringType(), True),\n",
    "    StructField(\"shipment_fee\", FloatType(), True),\n",
    "    StructField(\"shipment_location_lat\", FloatType(), True),\n",
    "    StructField(\"shipment_location_long\", FloatType(), True),\n",
    "    StructField(\"total_amount\", FloatType(), True),\n",
    "    StructField(\"clear_payment\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for customer_session.csv\n",
    "customer_session_schema = StructType([\n",
    "    StructField(\"session_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for fraud_transaction.csv\n",
    "fraud_transaction_schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"is_fraud\", BooleanType(), True)  \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer DataFrame Schema:\n",
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- birthdate: date (nullable = true)\n",
      " |-- first_join_date: date (nullable = true)\n",
      "\n",
      "\n",
      "Category DataFrame Schema:\n",
      "root\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- cat_level1: string (nullable = true)\n",
      " |-- cat_level2: string (nullable = true)\n",
      " |-- cat_level3: string (nullable = true)\n",
      "\n",
      "\n",
      "Browsing Behaviour DataFrame Schema:\n",
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- traffic_source: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      "\n",
      "\n",
      "Product DataFrame Schema:\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- baseColour: string (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- usage: string (nullable = true)\n",
      " |-- productDisplayName: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      "\n",
      "\n",
      "Transaction DataFrame Schema:\n",
      "root\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- product_metadata: string (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- payment_status: string (nullable = true)\n",
      " |-- promo_amount: float (nullable = true)\n",
      " |-- promo_code: string (nullable = true)\n",
      " |-- shipment_fee: float (nullable = true)\n",
      " |-- shipment_location_lat: float (nullable = true)\n",
      " |-- shipment_location_long: float (nullable = true)\n",
      " |-- total_amount: float (nullable = true)\n",
      " |-- clear_payment: string (nullable = true)\n",
      "\n",
      "\n",
      "Customer Session DataFrame Schema:\n",
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      "\n",
      "\n",
      "Fraud Transaction DataFrame Schema:\n",
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- is_fraud: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "customer_csv_path = \"customer.csv\"\n",
    "category_csv_path = \"category.csv\"\n",
    "browsing_behaviour_csv_path = \"browsing_behaviour.csv\"\n",
    "product_csv_path = \"product.csv\"\n",
    "transaction_csv_path = \"transactions.csv\"\n",
    "customer_session_csv_path = \"customer_session.csv\"\n",
    "fraud_transaction_csv_path = \"fraud_transaction.csv\"\n",
    "\n",
    "# Load CSV files into DataFrames with predefined schemas\n",
    "df_customer = (spark.read.format(\"csv\")\n",
    "               .schema(customer_schema)\n",
    "               .option(\"header\", \"true\")\n",
    "               .load(customer_csv_path))\n",
    "\n",
    "df_category = (spark.read.format(\"csv\")\n",
    "               .schema(category_schema)\n",
    "               .option(\"header\", \"true\")\n",
    "               .load(category_csv_path))\n",
    "\n",
    "df_browsing_behaviour = (spark.read.format(\"csv\")\n",
    "                         .schema(browsing_behaviour_schema)\n",
    "                         .option(\"header\", \"true\")\n",
    "                         .load(browsing_behaviour_csv_path))\n",
    "\n",
    "df_product = (spark.read.format(\"csv\")\n",
    "              .schema(product_schema)\n",
    "              .option(\"header\", \"true\")\n",
    "              .load(product_csv_path))\n",
    "\n",
    "df_transaction = (spark.read.format(\"csv\")\n",
    "                  .schema(transaction_schema)\n",
    "                  .option(\"header\", \"true\")\n",
    "                  .load(transaction_csv_path))\n",
    "\n",
    "df_customer_session = (spark.read.format(\"csv\")\n",
    "                       .schema(customer_session_schema)\n",
    "                       .option(\"header\", \"true\")\n",
    "                       .load(customer_session_csv_path))\n",
    "\n",
    "df_fraud_transaction = (spark.read.format(\"csv\")\n",
    "                        .schema(fraud_transaction_schema)\n",
    "                        .option(\"header\", \"true\")\n",
    "                        .load(fraud_transaction_csv_path))\n",
    "\n",
    "# Print schemas for each DataFrame\n",
    "print(\"Customer DataFrame Schema:\")\n",
    "df_customer.printSchema()\n",
    "\n",
    "print(\"\\nCategory DataFrame Schema:\")\n",
    "df_category.printSchema()\n",
    "\n",
    "print(\"\\nBrowsing Behaviour DataFrame Schema:\")\n",
    "df_browsing_behaviour.printSchema()\n",
    "\n",
    "print(\"\\nProduct DataFrame Schema:\")\n",
    "df_product.printSchema()\n",
    "\n",
    "print(\"\\nTransaction DataFrame Schema:\")\n",
    "df_transaction.printSchema()\n",
    "\n",
    "print(\"\\nCustomer Session DataFrame Schema:\")\n",
    "df_customer_session.printSchema()\n",
    "\n",
    "print(\"\\nFraud Transaction DataFrame Schema:\")\n",
    "df_fraud_transaction.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- created_at: timestamp (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- product_metadata: string (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- payment_status: string (nullable = true)\n",
      " |-- promo_amount: float (nullable = true)\n",
      " |-- promo_code: string (nullable = true)\n",
      " |-- shipment_fee: float (nullable = true)\n",
      " |-- shipment_location_lat: float (nullable = true)\n",
      " |-- shipment_location_long: float (nullable = true)\n",
      " |-- total_amount: float (nullable = true)\n",
      " |-- clear_payment: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- is_fraud: boolean (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- birthdate: date (nullable = true)\n",
      " |-- first_join_date: date (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- baseColour: string (nullable = true)\n",
      " |-- season: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- usage: string (nullable = true)\n",
      " |-- productDisplayName: string (nullable = true)\n",
      " |-- cat_level1: string (nullable = true)\n",
      " |-- cat_level2: string (nullable = true)\n",
      " |-- cat_level3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, json_tuple\n",
    "\n",
    "# Extract product_id from product_metadata in df_transaction\n",
    "df_transaction_parsed = df_transaction.withColumn(\"product_id\", json_tuple(col(\"product_metadata\"), \"product_id\"))\n",
    "\n",
    "# Now, we join customer, product using 'id' from product, category, and fraud transaction static datasets\n",
    "# Join df_fraud_transaction on transaction_id, df_product on 'id', and df_category on category_id\n",
    "static_data_df = df_transaction_parsed.join(df_fraud_transaction, on='transaction_id', how='inner') \\\n",
    "                                      .join(df_customer, on='customer_id', how='inner') \\\n",
    "                                      .join(df_product, df_transaction_parsed.product_id == df_product.id, how='inner') \\\n",
    "                                      .join(df_category, on='category_id', how='inner')\n",
    "\n",
    "# Now we have included the 'is_fraud' column from df_fraud_transaction to identify fraudulent transactions\n",
    "\n",
    "static_data_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using the Kafka topics from the producer in Task 1, ingest the streaming data into Spark Streaming, assuming all data comes in the String format. Except for the 'ts' column, you shall receive it as an Int type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- event_time: string (nullable = true)\n",
      " |-- traffic_source: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- event_ts: timestamp (nullable = true)\n",
      "\n",
      "root\n",
      " |-- created_at: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- product_metadata: string (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- payment_status: string (nullable = true)\n",
      " |-- promo_amount: float (nullable = true)\n",
      " |-- promo_code: string (nullable = true)\n",
      " |-- shipment_fee: float (nullable = true)\n",
      " |-- shipment_location_lat: float (nullable = true)\n",
      " |-- shipment_location_long: float (nullable = true)\n",
      " |-- total_amount: float (nullable = true)\n",
      " |-- clear_payment: string (nullable = true)\n",
      " |-- event_ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json, col, from_unixtime, expr, to_timestamp\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType, FloatType, TimestampType\n",
    "\n",
    "# Kafka configurations\n",
    "kafka_browsing_topic = \"browsing_behaviour\"\n",
    "kafka_transaction_topic = \"transactions\"\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "\n",
    "# Define schema for browsing behavior\n",
    "browsing_schema = StructType() \\\n",
    "    .add(\"session_id\", StringType()) \\\n",
    "    .add(\"event_type\", StringType()) \\\n",
    "    .add(\"event_time\", StringType()) \\\n",
    "    .add(\"traffic_source\", StringType()) \\\n",
    "    .add(\"device_type\", StringType()) \\\n",
    "    .add(\"ts\", IntegerType())\n",
    "\n",
    "# Define schema for transactions\n",
    "transaction_schema = StructType() \\\n",
    "    .add(\"created_at\", StringType()) \\\n",
    "    .add(\"customer_id\", StringType()) \\\n",
    "    .add(\"transaction_id\", StringType()) \\\n",
    "    .add(\"session_id\", StringType()) \\\n",
    "    .add(\"product_metadata\", StringType()) \\\n",
    "    .add(\"payment_method\", StringType()) \\\n",
    "    .add(\"payment_status\", StringType()) \\\n",
    "    .add(\"promo_amount\", FloatType()) \\\n",
    "    .add(\"promo_code\", StringType()) \\\n",
    "    .add(\"shipment_fee\", FloatType()) \\\n",
    "    .add(\"shipment_location_lat\", FloatType()) \\\n",
    "    .add(\"shipment_location_long\", FloatType()) \\\n",
    "    .add(\"total_amount\", FloatType()) \\\n",
    "    .add(\"clear_payment\", StringType()) \\\n",
    "    .add(\"ts\", IntegerType())\n",
    "\n",
    "# Ingest browsing stream data from Kafka\n",
    "browsing_stream_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_browsing_topic) \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Ingest transaction stream data from Kafka\n",
    "transaction_stream_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_transaction_topic) \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Parse the browsing and transaction data using the schemas\n",
    "browsing_stream_df = browsing_stream_df.withColumn(\"value\", from_json(col(\"value\"), browsing_schema)) \\\n",
    "                                       .selectExpr(\"value.*\") \\\n",
    "                                       .withColumn(\"event_ts\", to_timestamp(from_unixtime(col(\"ts\")))) \\\n",
    "                                       .drop(\"ts\")\n",
    "\n",
    "transaction_stream_df = transaction_stream_df.withColumn(\"value\", from_json(col(\"value\"), transaction_schema)) \\\n",
    "                                             .selectExpr(\"value.*\") \\\n",
    "                                             .withColumn(\"event_ts\", to_timestamp(from_unixtime(col(\"ts\")))) \\\n",
    "                                             .drop(\"ts\")\n",
    "\n",
    "# Print schemas to verify\n",
    "browsing_stream_df.printSchema()\n",
    "transaction_stream_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.\tThen, the streaming data format should be transformed into the proper formats following the metadata file schema, similar to assignment 2A. Perform the following tasks:  \n",
    "a)\tFor the 'ts' column, convert it to the timestamp format, we will use it as event_ts.  \n",
    "b)\tIf the data is late for more than 2 minutes, discard it.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Current timestamp\n",
    "current_timestamp_expr = expr(\"current_timestamp()\")\n",
    "\n",
    "# Filtering out late data (late by more than 2 minutes) with watermarking\n",
    "browsing_stream_df = browsing_stream_df.withWatermark(\"event_ts\", \"2 minutes\") \\\n",
    "    .filter(col(\"event_ts\") >= current_timestamp_expr - expr(\"INTERVAL 2 MINUTES\"))\n",
    "\n",
    "transaction_stream_df = transaction_stream_df.withWatermark(\"event_ts\", \"2 minutes\") \\\n",
    "    .filter(col(\"event_ts\") >= current_timestamp_expr - expr(\"INTERVAL 2 MINUTES\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.\tAggregate the streaming data frames and create features you used in your assignment 2A model.  \n",
    "(note: customer ID has already been included in the stream.) Then, join the static data frames with the streaming data frame as our final data for prediction. Perform data type/column conversion according to your ML model and print out the Schema. (Again, you can reuse code from A2A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- product_metadata: string (nullable = true)\n",
      " |-- payment_method: string (nullable = true)\n",
      " |-- payment_status: string (nullable = true)\n",
      " |-- promo_amount: float (nullable = true)\n",
      " |-- shipment_fee: float (nullable = true)\n",
      " |-- total_amount: float (nullable = true)\n",
      " |-- event_ts: timestamp (nullable = true)\n",
      " |-- is_fraud: boolean (nullable = true)\n",
      "\n",
      "root\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- fraud_count: long (nullable = false)\n",
      "\n",
      "root\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- total_quantity: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json, col, from_unixtime, expr, to_timestamp, window, count, sum as F_sum, json_tuple\n",
    "\n",
    "# Join transaction stream with fraud static data\n",
    "transaction_with_fraud_df = transaction_stream_df \\\n",
    "    .join(df_fraud_transaction, on=\"transaction_id\", how=\"inner\") \\\n",
    "    .select(\"transaction_id\", \"customer_id\", \"session_id\", \"product_metadata\", \"payment_method\", \n",
    "            \"payment_status\", \"promo_amount\", \"shipment_fee\", \"total_amount\", \"event_ts\", \"is_fraud\")\n",
    "\n",
    "# Verify schema after join\n",
    "transaction_with_fraud_df.printSchema()\n",
    "\n",
    "# Extract product details from 'product_metadata'\n",
    "product_extracted_df = transaction_with_fraud_df.withColumn(\"product_id\", json_tuple(col(\"product_metadata\"), \"product_id\")) \\\n",
    "                                                .withColumn(\"quantity_str\", json_tuple(col(\"product_metadata\"), \"quantity\"))\n",
    "\n",
    "# Cast 'quantity' to integer\n",
    "product_extracted_df = product_extracted_df.withColumn(\"quantity\", col(\"quantity_str\").cast(\"int\")).drop(\"quantity_str\")\n",
    "\n",
    "# Aggregate fraud count every 10 seconds\n",
    "fraud_count_df = product_extracted_df \\\n",
    "    .groupBy(window(col(\"event_ts\"), \"10 seconds\"), col(\"is_fraud\")) \\\n",
    "    .agg(count(\"transaction_id\").alias(\"fraud_count\")) \\\n",
    "    .filter(col(\"is_fraud\") == True) \\\n",
    "    .select(\"window\", \"fraud_count\")\n",
    "\n",
    "# Print schema for verification\n",
    "fraud_count_df.printSchema()\n",
    "\n",
    "# Write fraud count data to Parquet file\n",
    "fraud_parquet_path = \"/tmp/fraud_parquet\"\n",
    "fraud_checkpoint_path = \"/tmp/fraud_checkpoint\"\n",
    "\n",
    "fraud_count_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", fraud_parquet_path) \\\n",
    "    .option(\"checkpointLocation\", fraud_checkpoint_path) \\\n",
    "    .start()\n",
    "\n",
    "# Calculating cumulative sales for non-fraud transactions \n",
    "cumulative_sales_df = product_extracted_df \\\n",
    "    .filter(col(\"is_fraud\") == False) \\\n",
    "    .groupBy(window(col(\"event_ts\"), \"30 seconds\"), col(\"product_id\")) \\\n",
    "    .agg(F_sum(col(\"quantity\")).alias(\"total_quantity\"))\n",
    "\n",
    "# Print schema for verification\n",
    "cumulative_sales_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.\tRead the two parquet files from task 7 as data streams and send to Kafka topics with appropriate names.\n",
    "(Note: You shall read the parquet files as a streaming data frame and send messages to the Kafka topic when new data appears in the parquet file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0xffff8755b520>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write cumulative sales data to Parquet file\n",
    "sales_parquet_path = \"/tmp/sales_parquet\"\n",
    "sales_checkpoint_path = \"/tmp/sales_checkpoint\"\n",
    "\n",
    "cumulative_sales_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", sales_parquet_path) \\\n",
    "    .option(\"checkpointLocation\", sales_checkpoint_path) \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- window: struct (nullable = true)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- total_quantity: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "# Define the schema for the cumulative sales Parquet file\n",
    "sales_schema = StructType([\n",
    "    StructField(\"window\", StructType([\n",
    "        StructField(\"start\", TimestampType(), True),\n",
    "        StructField(\"end\", TimestampType(), True)\n",
    "    ])),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"total_quantity\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Define the path to the saved sales Parquet file\n",
    "sales_parquet_path = \"/tmp/sales_parquet\"\n",
    "\n",
    "# Read the cumulative sales Parquet file as a stream using the defined schema\n",
    "sales_stream_df = spark \\\n",
    "    .readStream \\\n",
    "    .schema(sales_schema) \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", sales_parquet_path) \\\n",
    "    .load()\n",
    "\n",
    "# Verify the schema of the sales streaming DataFrame\n",
    "sales_stream_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_json, struct\n",
    "\n",
    "# Convert the columns into a JSON format for Kafka\n",
    "sales_stream_for_kafka = sales_stream_df \\\n",
    "    .selectExpr(\"to_json(struct(*)) AS value\")\n",
    "\n",
    "# Verify the structure before sending to Kafka\n",
    "sales_stream_for_kafka.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.\tWrite a Parquet file and save the following data frames (tip: you may look at part 3 and think about what columns to save):  \n",
    "a.\tPersist the raw data from 6a in parquet format. Every student may have different features/columns in their data frames depending on their model, at the bare minimum, we need some IDs to identify those frauds later on (transaction_id and/or session_id). After that, read the parquet file and show a few rows to verify it is saved correctly.  \n",
    "b.\tPersist the data from 6b in another parquet file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0xffff87558850>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kafka topic and configurations\n",
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "kafka_sales_topic = \"cumulative_sales\"\n",
    "\n",
    "# Send the stream data to Kafka\n",
    "sales_stream_for_kafka \\\n",
    "    .writeStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"topic\", kafka_sales_topic) \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/sales_kafka_checkpoint\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.\tThe company is interested in the number of potential frauds as they happen and the products in customersâ€™ shopping carts (so that they can plan their stock level ahead.) Load your ML model, and use the model to predict/process each browsing session/transaction as follows:  \n",
    "a)\tEvery 10 seconds, show the total number of potential frauds (prediction = 1) in the last 2 minutes, and persist the raw data (see 7a).  \n",
    "b)\tEvery 30 seconds, find the top 20 products (order by quantity descending) in the last 30 seconds, show product ID, name and total quantity. We only need the non-fraud transactions (prediction=0) by extracting customer shopping cart details (sum of all items of ADD_TO_CART(ATC) events from browsing behaviour, you can also extract it from transactions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
