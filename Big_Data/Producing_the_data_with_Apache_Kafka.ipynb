{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Producing the data\n",
    "In this task, we will implement Apache Kafka producers to simulate real-time data streaming. Spark is not allowed in this part since it’s simulating a streaming data source.  \n",
    "\n",
    "1.\tYour program should send one batch of browsing behaviour data every 5 seconds. One batch consists of a random 500-1000 rows from the browsing behaviour dataset. The CSV shouldn’t be loaded to memory at once to conserve memory (i.e. Read row as needed). Keep track of the start and end event_time. (You can assume the dataset is sorted by event_time.)  \n",
    "2.\tAdd an integer column named ‘ts’ for each row, a Unix timestamp in seconds since the epoch. Spead your batch out evenly for 5 seconds.  \n",
    "a.\tFor example, if you send a batch of 600 records at 2023-09-01 00:00:00 (ISO format: YYYY-MM-DD HH:MM:SS) -> (ts = 1693526400):  \n",
    "-\tRecord 1-120: ts = 1693526400   \n",
    "-\tRecord 121-240: ts = 1693526401   \n",
    "-\tRecord 241-360: ts = 1693526402  \n",
    "-\t….  \n",
    "3.\tRead the transactions between the start and end event_time in 1.1 every 5 seconds (the same frequency as browsing behaviour) and create a batch.  \n",
    "4.\tSend your two batches from 1.1 and 1.3 to Kafka topics with an appropriate name.  \n",
    "Note 1: In 1.1, “random 500-1000” means the number of rows is random, and the data file is still read sequentially.  \n",
    "Note 2: All the data except for the ‘ts’ column should be sent in the original String type without changing to any other type. This is because we are simulating a streaming access log and need to reduce the required processing at the source.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent 672 records to browsing_behaviour\n",
      "Sent 751 records to transactions\n",
      "Sent 565 records to browsing_behaviour\n",
      "Sent 870 records to transactions\n",
      "Sent 714 records to browsing_behaviour\n",
      "Sent 971 records to transactions\n",
      "Sent 533 records to browsing_behaviour\n",
      "Sent 767 records to transactions\n",
      "Sent 738 records to browsing_behaviour\n",
      "Sent 926 records to transactions\n",
      "Sent 918 records to browsing_behaviour\n",
      "Sent 530 records to transactions\n",
      "Sent 590 records to browsing_behaviour\n",
      "Sent 825 records to transactions\n",
      "Sent 865 records to browsing_behaviour\n",
      "Sent 635 records to transactions\n",
      "Sent 660 records to browsing_behaviour\n",
      "Sent 552 records to transactions\n",
      "Sent 833 records to browsing_behaviour\n",
      "Sent 734 records to transactions\n",
      "Sent 575 records to browsing_behaviour\n",
      "Sent 538 records to transactions\n",
      "Sent 849 records to browsing_behaviour\n",
      "Sent 921 records to transactions\n",
      "Sent 943 records to browsing_behaviour\n",
      "Sent 892 records to transactions\n",
      "Sent 796 records to browsing_behaviour\n",
      "Sent 696 records to transactions\n",
      "Sent 835 records to browsing_behaviour\n",
      "Sent 575 records to transactions\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "from time import sleep\n",
    "from kafka3 import KafkaProducer\n",
    "from datetime import datetime\n",
    "\n",
    "# Kafka configuration\n",
    "hostip = \"kafka\"\n",
    "browsing_topic = \"browsing_behaviour\"\n",
    "transaction_topic = \"transactions\"\n",
    "\n",
    "# Function to connect to Kafka producer\n",
    "def connect_kafka_producer():\n",
    "    try:\n",
    "        producer = KafkaProducer(bootstrap_servers=[f'{hostip}:9092'],\n",
    "                                 value_serializer=lambda x: x.encode('utf-8'),\n",
    "                                 api_version=(0, 10))\n",
    "        return producer\n",
    "    except Exception as ex:\n",
    "        print(\"Error connecting to Kafka:\", str(ex))\n",
    "        return None\n",
    "\n",
    "# Function to add 'ts' and send batch data to Kafka\n",
    "def send_to_kafka(producer, topic, data, start_ts):\n",
    "    batch_size = len(data)\n",
    "    rows_per_second = batch_size // 5\n",
    "    remainder = batch_size % 5\n",
    "\n",
    "    # Loop through the records and send them to Kafka\n",
    "    for i, row in enumerate(data):\n",
    "        current_ts = start_ts + (i // rows_per_second)\n",
    "        if i % rows_per_second == 0 and remainder > 0:\n",
    "            current_ts += 1\n",
    "            remainder -= 1\n",
    "        \n",
    "        row['ts'] = current_ts\n",
    "        # Send each row to Kafka\n",
    "        producer.send(topic, value=','.join([str(v) for v in row.values()]))\n",
    "    \n",
    "    # Print the number of records sent instead of individual records\n",
    "    print(f\"Sent {len(data)} records to {topic}\")\n",
    "\n",
    "# Read CSV in a memory-efficient way and fetch random rows\n",
    "def read_random_batch_from_csv(file_path, batch_size):\n",
    "    selected_rows = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for i, row in enumerate(reader):\n",
    "            if len(selected_rows) < batch_size:\n",
    "                selected_rows.append(row)\n",
    "            else:\n",
    "                break\n",
    "    return selected_rows\n",
    "\n",
    "# Main function to simulate the browsing and transaction data stream\n",
    "def simulate_data_streams(browsing_file, transaction_file):\n",
    "    producer = connect_kafka_producer()\n",
    "    if producer is None:\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        # Generate random batch size between 500-1000 for browsing behavior\n",
    "        browsing_batch_size = random.randint(500, 1000)\n",
    "        browsing_batch = read_random_batch_from_csv(browsing_file, browsing_batch_size)\n",
    "\n",
    "        # Generate random batch size between 500-1000 for transactions\n",
    "        transaction_batch_size = random.randint(500, 1000)\n",
    "        transaction_batch = read_random_batch_from_csv(transaction_file, transaction_batch_size)\n",
    "\n",
    "        # Current timestamp\n",
    "        current_time = datetime.now()\n",
    "\n",
    "        # Convert to Unix timestamp\n",
    "        start_ts = int(current_time.timestamp())\n",
    "\n",
    "        # Send browsing data batch to Kafka\n",
    "        send_to_kafka(producer, browsing_topic, browsing_batch, start_ts)\n",
    "\n",
    "        # Send transaction data batch to Kafka\n",
    "        send_to_kafka(producer, transaction_topic, transaction_batch, start_ts)\n",
    "\n",
    "        # Sleep for 5 seconds to simulate real-time streaming\n",
    "        sleep(5)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # File paths to the browsing behaviour and transaction datasets\n",
    "    browsing_file = 'browsing_behaviour.csv'\n",
    "    transaction_file = 'transactions.csv'\n",
    "    \n",
    "    simulate_data_streams(browsing_file, transaction_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
