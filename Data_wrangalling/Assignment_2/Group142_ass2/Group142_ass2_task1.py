# -*- coding: utf-8 -*-
"""Group142_ass2_task1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ttkiNapqaJzHrXcmk5lbKyW3AB79NBLE

# FIT5196-S2-2024 Assessment 2: Task 1 Data Cleansing
### Group ID: Group_142
#### Student Name: Pankaj Shitole
#### Student ID: (33570523)
#### Student Name:Sachin Shivaramaiah
#### Student ID: (34194037)

# Introduction:

#### - Task 1 focuses on cleaning and preparing the dataset for analysis by addressing three key areas: cleaning inconsistent data, handling missing values, and rectifying outliers.
#### - The process involves identifying and correcting inconsistencies in various fields, ensuring that missing data is accurately filled, and resolving any outliers. These steps are crucial in ensuring data accuracy and consistency, which are essential for reliable analysis in subsequent tasks.

# Importing Required Libraries.
"""

import pandas as pd
import numpy as np
from datetime import datetime
from math import *
import ast
import re
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import pandas as pd
import nltk
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

# Download vader lexicon for SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

"""# 1. Reading and Understanding File Structure.

* ### Here we will be understanding data set along with it's structure data types and see if we can find anything unsual while analysing the data sets

#### Load the datasets
"""

dirty_data = pd.read_csv('Group142_dirty_data.csv')

missing_data = pd.read_csv('Group142_missing_data.csv')

outlier_data = pd.read_csv('Group142_outlier_data.csv')

# Display the dirty data
dirty_data

"""#### Analysis of Dirty Data
- Check any potential anomaly after displaying head and tail of the dataframe
"""

dirty_data.head(5)

dirty_data.tail(5)

"""* #### Anamoly in Date Column
In the above data frame in row 3 and 499 we see the dates are 09-09-2019 and 02-10-2019. However for other rows the format is YYYY-mm-dd
"""

# Display the summary statistics
dirty_data.describe()

"""# 2. Rectifying the Anomalies in dirty data

### 2.1 Strating with Date column

#### 2.1.1 Identifying anomalies.
"""

dirty_data.dtypes

"""From the above analysis verifying the dates"""

dirty_data['date'][1]

dirty_data['date'][3]

dirty_data['date'][499]

"""#### 2.1.2 Implemented Methodology:

* Alongside correcting the date column, we will ensure that the seasons are correctly mapped to their corresponding months. In Australia, the seasons are defined as follows

    -  Spring - the three transition months September, October and November.
    -  Summer - the three hottest months December, January and February.
    -  Autumn - the transition months March, April and May.
    -  Winter - the three coldest months June, July and August.

#### 2.1.3 Implemented Code:

##### Step 1: Check for indices with incorrect date format
- Below, we define a function that accounts for all possible date formats.
- This function parses each row, converting dates to a standard format.
- If the previous date format does not match with the modified date format, the row's index is added to a list, and the date is replaced with the correct standard format
"""

# List to store the indices of rows where the date format was modified
modified_indices = []

# Function to parse and reformat dates
def format_date(date_str, idx):
    formats = ["%Y-%m-%d", "%Y-%d-%m", "%d-%m-%Y", "%m-%d-%Y", "%m/%d/%Y", "%Y/%m/%d", "%d/%m/%Y"]
    original_str = date_str  # Save the original date string
    for fmt in formats:
        try:
            # Try to parse the date using each format in the list
            date_object = datetime.strptime(date_str, fmt)
            # If successful, format it to 'YYYY/MM/DD'
            formatted_str = date_object.strftime('%Y-%m-%d')

            # Check if the format has been changed
            if original_str != formatted_str:
                modified_indices.append(idx)  # Store the index of the modified row
            return formatted_str
        except ValueError:
            continue
    return pd.NaT

# Apply the function to the 'date' column
dirty_data['date'] = dirty_data.apply(lambda row: format_date(row['date'], row.name), axis=1)

# Print the modified indices
print("Indices of modified rows:", modified_indices)

# Check for null values if any due to some error
dirty_data['date'].isna().sum()

# Display the dataframe
dirty_data

"""##### Step 2: Extract the month from the dates"""

dirty_data['month'] = pd.to_datetime(dirty_data['date'], errors='coerce').dt.month_name()

dirty_data[['date', 'month']]

dirty_data['month'].isna().sum()

"""##### Step 3: Checking if respective months is being mapped for respective seasons
- The code below checks whether the months and their corresponding seasons are correctly aligned in the dataset.
- It compares the assigned season for each month against a predefined dictionary of seasons and months.
- If a mismatch is found, the season is corrected, and the row's index is recorded.
- Finally, the code prints out the indexes of any rows with incorrect season-month combinations.
"""

# Define a dictionary where the keys are seasons and the values are lists of corresponding months
seasons_dict = {
    'Spring': ['September', 'October', 'November'],
    'Summer': ['December', 'January', 'February'],
    'Autumn': ['March', 'April', 'May'],
    'Winter': ['June', 'July', 'August']
}

# List to store incorrect indexes
incorrect_indexes = []

# Iterate through each row of the DataFrame
for index, row in dirty_data.iterrows():
    month = row['month']
    season = row['season']

    # Check if the month is in the correct list of months for that season
    if month not in seasons_dict.get(season, []):
        # If not, store the index
        incorrect_indexes.append(index)
        # Correct the season
        for correct_season, months in seasons_dict.items():
            if month in months:
                dirty_data.at[index, 'season'] = correct_season
                break

# Print the incorrect indexes
if incorrect_indexes:
    print(f"Rows with incorrect season-month combinations: {incorrect_indexes}")
else:
    print("All season-month combinations are correct.")

# Check the length of incorrect indexes in season-month combination
len(incorrect_indexes)

# Check the length of modified indices for the date
len(modified_indices)

"""##### Step 4: Validation
##### Now we Validate if there are any overlapping index, since we know that one row has only one anomalies.
* This code checks if there are any overlapping values between two lists (modified_indices and incorrect_indexes). If a match is found, it prints the matching index. Since there are no common elements, the second part of the code merges both lists by adding incorrect_indexes to modified_indices.
"""

for i in modified_indices:
    for j in incorrect_indexes:
        if i==j:
            print(i)

"""Here we have not got any similar rows."""

# There are no similar indices so merge the lists
modified_indices += incorrect_indexes

len(modified_indices)

"""#### Best practice to keep things on track: Flagging Modified Rows for Easy Tracking
* The purpose of this code is to track which rows in the dataset have been modified during the cleaning process. By creating the 'modified_row' column, the code provides a clear indicator of which rows were updated, allowing for easy identification of changes. This is helpful for auditing and reviewing the data cleaning steps, ensuring transparency in the modifications made and making it simpler to revisit or analyze those rows later on. Essentially, it acts as a flagging system to keep a record of all altered rows in the dataset.
"""

# Create the new 'modified_row' column, setting 1 if the row was modified, else 0
dirty_data['modified_row'] = dirty_data.index.map(lambda idx: 1 if idx in modified_indices else 0)

dirty_data

dirty_data['modified_row'].sum()

# Drop the created month column from the dataframe
dirty_data.drop(columns=['month'], inplace=True)

"""#### Note to be Considered.

In the specs it's given that "The below columns are error-free (i.e. don’t look for any errors in dirty data for them)":
- coupon_discount
- delivery_charges
- The ordered quantity values in the shopping_cart attribute
- order_id
- customer_id
- latest_customer_review

### 2.2 Customer_lat and Customer_long columns

#### 2.2.1 Identifying anomalies through summary statistics.

* From the below table we can see that there is varition in min and max values of latitude and longitude, it seems like few of the rows values between these 2 columns are swapped.
"""

dirty_data.describe()

"""#### 2.2.2 Implemented Methodology:

* Define valid ranges for latitude (-90 to 90) and longitude (-180 to 180). [Reference: https://help.arcgis.com/en/geodatabase/10.0/sdk/arcsde/concepts/geometry/coordref/coordsys/geographic/geographic.htm#]
* Check each row: A function (check_lat_long) checks if the customer_lat and customer_long values are within the valid range.
* Flag invalid rows: If values are outside the defined range, the row index is added to the invalid_lat_long_indices list.
* Swap incorrect values: Another function (swap_lat_long) corrects invalid entries by swapping latitude and longitude values for the flagged rows.

#### 2.2.3 Implemented Code
"""

# Define the valid ranges for latitude and longitude
LATITUDE_MIN = -90
LATITUDE_MAX = 90
LONGITUDE_MIN = -180
LONGITUDE_MAX = 180

invalid_lat_long_indices = []

# Function to check if latitude and longitude are within their respective ranges
def check_lat_long(row):
    lat, long = row['customer_lat'], row['customer_long']
    if not (LATITUDE_MIN <= lat <= LATITUDE_MAX) or not (LONGITUDE_MIN <= long <= LONGITUDE_MAX):
        # If latitude or longitude is outside the valid range, append the row index to the list
        invalid_lat_long_indices.append(row.name)

# Apply the function to each row
dirty_data.apply(check_lat_long, axis=1)

# Print the list of invalid indices
print("Indices with invalid latitude/longitude:", invalid_lat_long_indices)

# Function to swap lat and long for the specified rows
def swap_lat_long(row):
    if row.name in invalid_lat_long_indices:
        # Swap the values of customer_lat and customer_long
        row['customer_lat'], row['customer_long'] = row['customer_long'], row['customer_lat']
        # Increment the 'modified_row' value by 1
        row['modified_row'] += 1
    return row

# Apply the swap function to the DataFrame
dirty_data = dirty_data.apply(swap_lat_long, axis=1)

# Verify that the lat and long have been swapped for the rows
print("Swapped latitude and longitude values for the specified rows:")
print(dirty_data.loc[invalid_lat_long_indices, ['customer_lat', 'customer_long']])

"""Check the summary statistics again to verify the changes"""

dirty_data.describe()

"""#### 2.2.4 Validation to confirm every rows has only one anomaly.

* The code below provides key statistics about the modified_row column, The minimum (0) and maximum (1) confirm that no rows were modified more than once, as expected. This ensures that the data cleaning process did not inadvertently over-correct any rows.
"""

# Check if any row is modified twice
dirty_data['modified_row'].describe()

# Check the number of rows modified
dirty_data['modified_row'].sum()

"""### 2.3 Calculte the distance to the nearest warehouse from the customer

#### 2.3.1 Implemented Methodology:

- Since we have exact co-ordinates of the customers we can calculte the distance of the nearest warehouse using warehouse.csv file
* Haversine Formula Calculation:
The Haversine formula is used to calculate the distance between two geographical points (latitude and longitude). In this case, it's used to calculate the distance from each customer's location to all available warehouses. The formula accounts for the curvature of the Earth to provide accurate distances in kilometers. [Ref: https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points]

* Nearest Warehouse Identification:
For each customer in the dataset, the function find_nearest_warehouse iterates through all the warehouse locations, calculating the distance between the customer's coordinates and the warehouse coordinates using the Haversine formula.
The warehouse with the shortest distance is identified as the nearest warehouse, and both the warehouse name and the calculated distance are stored.

* Calculating and Comparing Values:
A new column for the calculated nearest warehouse and its distance is added to the dataset.
The actual nearest_warehouse and distance_to_nearest_warehouse in the dataset are compared with the newly calculated values to identify mismatches. A small buffer of 0.01 km is allowed for distance discrepancies.

* Flagging Mismatches:
If the calculated warehouse does not match the existing nearest_warehouse, the row index is stored in mismatched_warehouse_indices.
Similarly, if the calculated distance exceeds the allowed buffer from the recorded distance, the row index is stored in mismatched_distance_indices.

* Correcting Mismatches:
Rows with mismatches in the nearest warehouse or distance are updated with the correct calculated values from the Haversine formula.
These updates are tracked by incrementing the modified_row column for each row that has been corrected.

* Validation and Output:
After corrections, a summary of the modified_row column is generated to provide insights into how many rows were modified and whether any rows were modified more than once.

#### 2.3.2 Implemented Code:
"""

# Load the warehouse dataset
warehouse_data = pd.read_csv('warehouses.csv')

# Display the firt few rows
warehouse_data.head()

"""Here we just have 3 warehouses."""

dirty_data.columns

# Function to calculate the distance between 2 geographical points
def haversine(lon1, lat1, lon2, lat2):
    # Convert decimal degrees to radians
    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])

    # Haversine formula
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
    c = 2 * asin(sqrt(a))
    r = 6378  # Radius of Earth in kilometers as per your requirement
    return c * r

# Function to calculate the nearest warehouse for a customer
def find_nearest_warehouse(customer_row):
    customer_lat = customer_row['customer_lat']
    customer_long = customer_row['customer_long']

    # Initialize variables to store the nearest warehouse and its distance
    min_distance = float('inf')
    nearest_warehouse = None

    # Iterate through each warehouse and calculate the distance
    for _, warehouse_row in warehouse_data.iterrows():
        warehouse_lat = warehouse_row['lat']
        warehouse_long = warehouse_row['lon']
        warehouse_name = warehouse_row['names']

        # Calculate distance using the Haversine formula
        distance = haversine(customer_long, customer_lat, warehouse_long, warehouse_lat)

        # Update if this warehouse is closer than the previous ones
        if distance < min_distance:
            min_distance = distance
            nearest_warehouse = warehouse_name

    # Return the name of the nearest warehouse and the distance
    return nearest_warehouse, min_distance

# Add a column 'calculated_nearest_warehouse' and 'calculated_distance_to_warehouse'
dirty_data[['calculated_nearest_warehouse', 'calculated_distance_to_warehouse']] = dirty_data.apply(
    lambda row: pd.Series(find_nearest_warehouse(row)), axis=1)

"""##### Here tried distance buffer with different values. Compared to all 0.01km i.e., 10m looks pretty decent."""

# Lists to store indices where the calculated warehouse/distance differs from the existing values
mismatched_warehouse_indices = []
mismatched_distance_indices = []

# Just a buffer to compensate the difference between calculations
buffer = 0.01

# Compare the calculated nearest warehouse and distance with the existing ones
for index, row in dirty_data.iterrows():
    # Check for warehouse mismatch
    if row['calculated_nearest_warehouse'] != row['nearest_warehouse']:
        mismatched_warehouse_indices.append(index)

    # Check for distance mismatch within a tolerance range
    if abs(row['calculated_distance_to_warehouse'] - row['distance_to_nearest_warehouse']) > buffer:
        mismatched_distance_indices.append(index)

# Print the lists of mismatched indices for warehouses and distances
print("Mismatched warehouse indices:", mismatched_warehouse_indices)
print("Mismatched distance indices:", mismatched_distance_indices)

len(mismatched_warehouse_indices)

len(mismatched_distance_indices)

for index in mismatched_distance_indices:
    for ind in mismatched_warehouse_indices:
        if index == ind:
            print(index)

"""**Display all the mismatch errors in nearest_warehouse and distnace_to_nearest_warehouse.**"""

dirty_data.loc[mismatched_warehouse_indices+mismatched_distance_indices][['nearest_warehouse','distance_to_nearest_warehouse','calculated_nearest_warehouse','calculated_distance_to_warehouse']]

# Update the 'nearest_warehouse' column for rows with mismatched nearest warehouse
dirty_data.loc[mismatched_warehouse_indices, 'nearest_warehouse'] = dirty_data.loc[mismatched_warehouse_indices, 'calculated_nearest_warehouse']

# Update the 'distance_to_nearest_warehouse' column for rows with mismatched distances, rounding to 4 decimal places
dirty_data.loc[mismatched_distance_indices, 'distance_to_nearest_warehouse'] = dirty_data.loc[mismatched_distance_indices, 'calculated_distance_to_warehouse'].round(4)

# Print the updated DataFrame
print("Updated DataFrame:")
dirty_data

# Combining both mismatched warehouse and distance indices
all_mismatched_indices = list(mismatched_warehouse_indices + mismatched_distance_indices)

# Update the 'modified_row' column for all mismatched rows
dirty_data.loc[all_mismatched_indices, 'modified_row'] += 1

"""Validate if no two anomalies are modified in the same row or not."""

dirty_data['modified_row'].describe()

dirty_data['modified_row'].sum()

"""### 2.4 Validating nearest warehouse names

#### 2.4.1 Implemented Methodology

* Initial Check: The frequency of each warehouse name is counted using value_counts() to assess how many rows are assigned to each warehouse before making any corrections.
* Mapping Inconsistent Names: A dictionary is created to map incorrect or inconsistent warehouse names like  lowercase versions to the correct, properly formatted versions.
* Row Iteration: The dataset is looped through row by row. If a row contains an incorrect warehouse name (based on the dictionary), the name is corrected.
* Flagging Modifications: For each corrected row, the modified_row column is updated to reflect that the row has been modified, allowing for tracking of changes.
* Post-Correction Validation: After all corrections, value_counts() is used again to ensure the warehouse names are now consistent across the dataset.
* Tracking Modifications: A summary of the modified_row column is generated to show how many rows were modified and whether any rows were modified more than once.
* Final Validation: The total number of modified rows is calculated to verify the extent of the changes made to the dataset.

#### 2.4.2 Implemented Code
"""

dirty_data['nearest_warehouse'].value_counts()

# Dictionary to map incorrect warehouse names to the correct ones
warehouse_corrections = {
    'thompson': 'Thompson',
    'nickolson': 'Nickolson',
    'bakers': 'Bakers'
}

# Iterate through the DataFrame and fix the warehouse names
for index, row in dirty_data.iterrows():
    warehouse = row['nearest_warehouse']

    if warehouse in warehouse_corrections:
        # If the warehouse name is incorrect, update the 'nearest_warehouse' with the correct name
        dirty_data.at[index, 'nearest_warehouse'] = warehouse_corrections[warehouse]

        # Increment the 'modified_row' column by 1 for the corrected row
        dirty_data.at[index, 'modified_row'] += 1

dirty_data['nearest_warehouse'].value_counts()

dirty_data['modified_row'].describe()

# Check the number of rows modified
dirty_data['modified_row'].sum()

"""### 2.5 Calculate Unit prices

#### 2.5.1 Implemented Methodology

- It is given in the specs that, shopping_cart quantities are correct. But they never mentioned about unit prices.
- So use missing data to calculate the unit prices, since it is a data without anomaly apart from coverage anomalies. It just contains missing values.
- Here in the missing data we have some null values for order_price, so filter out the data with null values and only consider non null values for getting the unit prices using multivariate linear equation system.
- We can use that data and pass it to linalg to compute the unit prices.
"""

missing_data.info()

"""- Here the shopping cart is without any missing value. But there are some missing values in order_price. <br>
- So we can consider only non-null rows and pass it to linalg to compute the unit prices in shopping cart.

#### 2.5.2 Implemented Code
"""

# Print the first few rows of the cart
list(missing_data['shopping_cart'].head())

missing_data['shopping_cart'][0]

# Filter out rows where 'order_price' is NaN
filtered_data = missing_data.dropna(subset=['order_price'])

# Convert the 'shopping_cart' from a string to an actual list of tuples
filtered_data['parsed_cart'] = filtered_data['shopping_cart'].apply(ast.literal_eval)

filtered_data['parsed_cart'].head()

# Find all unique items from the shopping cart data
all_items = set()
for cart in filtered_data['parsed_cart']:
    for item, quantity in cart:
        all_items.add(item)

all_items

"""As the specification says, 'The retail store focuses only on 10 branded items and sells them at competitive prices', the unique count of the items is correct."""

# Sort the items to keep them in a consistent order
sorted_items = sorted(all_items)

"""Here we are creating a quantity_matrix with all the quantity values for the given cart row. So basically we are creating a system of linear equations like, <br>
$A.X = B$ <br>
where $A$ is quantity matrix<br>
$X$ is unit names<br>
$B$ is order price for the given row
"""

# Build the matrix where each row is an order and each column is an item
# The matrix holds the quantity of each item for each order
quantity_matrix = []
for cart in filtered_data['parsed_cart']:
    row = [0] * len(sorted_items)  # Initialize row with zero quantities
    for item, quantity in cart:
        item_index = sorted_items.index(item)  # Find the position of the item
        row[item_index] = quantity  # Set the quantity of the item in this order
    quantity_matrix.append(row)

quantity_matrix = np.array(quantity_matrix)  # Convert the list to a NumPy array

quantity_matrix

quantity_matrix.shape

# Extract the order prices as the target variable
order_totals = filtered_data['order_price'].values

"""We are using lstsq instead of linalg.solve because solve mostly works well with square matrix. [Ref: https://realpython.com/python-linear-algebra/]"""

# Use least squares to calculate the unit prices for each item
unit_prices, residuals, rank, s = np.linalg.lstsq(quantity_matrix, order_totals, rcond=None)

# Create a dictionary to store unit prices for each item
unit_prices_dict = {sorted_items[i]: round(unit_prices[i]) for i in range(len(sorted_items))}

# Display the dictionary of unit prices
print("Unit Prices Dictionary:")
print(unit_prices_dict)

# Display how far the calculated prices are from actual prices
print(f"\nResiduals (sum of squared errors): {residuals}")

"""### 2.6 Validating Shopping_cart, order_price and order_total

#### 2.6.1. Implemented Methodology

This code iterates through each row in the dirty_data DataFrame to identify and correct inconsistencies in order calculations. It focuses on three key components: the order_price, order_total, and the shopping_cart since they are correlated.
- For each row, it first parses the shopping_cart and calculates the expected order_price by summing up the product of unit prices and quantities from the cart. And for that it uses the unit prices which we have calculated in the previous section. It then calculates the expected order_total by applying the formula:

$order total = order price * (1 - row['coupon discount'] / 100) + row['delivery charges']$

- The code then checks for anomalies, adhering to the rule that only one column can be wrong per row and the logic is with conditional statements:

        - If the order_price matches but the order_total does not with the calculated ones, it assumes the order_total is incorrect, updates it with the calculated value, and records the index.
        - If the shopping_cart and order_total are correct but the order_price is not with the calculated values, it corrects the order_price using the calculated value and logs the index.
        - If both the calculated order_price and order_total do not match the recorded values, it concludes that the issue is with the cart and records the index.

#### 2.6.2. Implemented Code
"""

# Initialize lists to store indices of incorrect entries
incorrect_cart_indices = []
incorrect_order_price_indices = []
incorrect_order_total_indices = []

# Loop through each row in dirty_data to check for inconsistencies
for idx, row in dirty_data.iterrows():
    # Parse the shopping cart
    shopping_cart = ast.literal_eval(row['shopping_cart'])

    # Calculate the expected order price based on the unit prices
    calculated_order_price = sum(unit_prices_dict[item] * quantity for item, quantity in shopping_cart)

    # Calculate the expected order total: order_price * (1 - coupon_discount/100) + delivery_charges
    calculated_order_total = calculated_order_price * (1 - row['coupon_discount'] / 100) + row['delivery_charges']

    # Check for the anomaly based on the fact that only one column can be wrong per row
    if int(calculated_order_price == row['order_price']) and float(round(calculated_order_total, 2)) != float(round(row['order_total'], 2)):
        # Order price is correct, but order total is wrong
        incorrect_order_total_indices.append(idx)
        dirty_data.at[idx, 'order_total'] = calculated_order_total
        dirty_data.at[idx, 'modified_row'] = 1  # Mark as modified row

    elif int(calculated_order_price != row['order_price']) and float(round(calculated_order_total, 2)) == float(round(row['order_total'], 2)):
        # Cart and order total are correct, but order price is wrong
        incorrect_order_price_indices.append(idx)
        dirty_data.at[idx, 'order_price'] = calculated_order_price
        dirty_data.at[idx, 'modified_row'] = 1  # Mark as modified

    elif int(calculated_order_price) != int(row['order_price']) and float(round(calculated_order_total, 2)) != float(round(row['order_total'], 2)):
        # Both order price and order total are correct, but the cart must be wrong
        incorrect_cart_indices.append(idx)
        dirty_data.at[idx, 'modified_row'] = 1

    # Display the errors
    print(idx, calculated_order_price, row['order_price'], calculated_order_total, row['order_total'])

# Print the lists of incorrect indices
print(f"Incorrect Cart Indices: {incorrect_cart_indices}")
print(f"Incorrect Order Price Indices: {incorrect_order_price_indices}")
print(f"Incorrect Order Total Indices: {incorrect_order_total_indices}")

len(incorrect_cart_indices)

len(incorrect_order_price_indices)

len(incorrect_order_total_indices)

"""Check if we are not exceeding the constraint of 1 anomaly per row."""

# Check for modified rows, if there is any extra row modified
dirty_data['modified_row'].describe()

# Calculate total modified rows
dirty_data['modified_row'].sum()

"""#### 2.6.3. Modify the shopping_cart for the extracted indices

Explanation
Here we begins by converting the cart string to a list of tuples for easier manipulation. Then calculates the current total price of the cart:

- If the current price matches the given order_price, it returns the cart unchanged.
- If there is a discrepancy, it iterates through each item in the cart:
    - For each item, it calculates the remaining price after removing the current item.
    - It then checks if replacing the current item with another item from the unit_prices_dict can result in the total matching the target order_price.
    - If a suitable replacement is found, the incorrect item is swapped, and the modified cart is returned.
"""

dirty_data['shopping_cart'][97]

dirty_data['shopping_cart'][0]

# Shopping cart before modification
dirty_data.loc[incorrect_cart_indices, 'shopping_cart']

# Function to modify the shopping cart by replacing only one incorrect item
def replace_incorrect_items(order_items_str, order_price):
    # Convert the string representation of the cart to a list of tuples
    order_items = ast.literal_eval(order_items_str)

    print("Before correcting:")
    print(order_items)

    # Calculate the current price of the cart
    current_price = sum(unit_prices_dict[item] * quantity for item, quantity in order_items)
    print("Current price of the cart before modification:", current_price)

    # If the current price matches the expected order price, no changes are needed
    if current_price == order_price:
        print("No modification needed.")
        return str(order_items)  # Return the unchanged cart

    # Iterate over each item in the cart to find the incorrect one
    for i, (item, quantity) in enumerate(order_items):
        # Calculate the remaining price after removing the current item
        remaining_price = current_price - (unit_prices_dict[item] * quantity)

        # Check for a replacement item that matches the target order price
        for new_item, unit_price in unit_prices_dict.items():
            # Calculate the new total if we replace the current item with the new item
            new_total = remaining_price + (unit_price * quantity)

            # If the new total matches the order price, replace the item
            if new_total == order_price:
                print(f"Replacing '{item}' with '{new_item}'")
                order_items[i] = (new_item, quantity)
                print("After correcting:")
                print(order_items)
                print(f"The given order price is {order_price}.\nAnd Current price of the cart after modification:{new_total}.\n")
                return str(order_items)  # Return the modified cart

    print("No suitable replacement found.")
    return str(order_items)  # Return the original cart if no match found

# Apply the replacement function to rows with incorrect carts
dirty_data.loc[incorrect_cart_indices, 'shopping_cart'] = dirty_data.loc[incorrect_cart_indices].apply(
    lambda row: replace_incorrect_items(row['shopping_cart'], row['order_price']), axis=1
)

# Shopping cart after modification
dirty_data.loc[incorrect_cart_indices, 'shopping_cart']

dirty_data['shopping_cart'][0]

"""### 2.6 Rectifying is_happy_customer

#### 2.6.1 Identifying irregularites.

Ref: https://medium.com/@gelsonm/to-use-or-lose-stop-words-in-nlp-de946edaa468
https://www.nltk.org/api/nltk.sentiment.html#nltk.sentiment.vader.SentimentIntensityAnalyzer.score_valence
"""

dirty_data['latest_customer_review'].info()

"""So we have one null value in the latest_customer_review. And it is given that if their is no review that means customer is new and by default it is happy. <br>
Check for is_happy_customer value for null review
"""

dirty_data[dirty_data['latest_customer_review'].isna()]

"""Based on the above output it is clear that is_happy_customer value for this row is correct and this row is already modified

#### 2.6.2 Implemented Methodology

* The code aims to validate and update the is_happy_customer field by analyzing the sentiment of customer reviews (latest_customer_review) using VADER sentiment analysis.
* If a review is missing, the customer is automatically assumed to be happy, and no sentiment analysis is performed.
* Reviews are preprocessed by removing special characters and converting the text to lowercase to ensure accurate sentiment analysis.
* A sentiment score is generated for each review. If the score indicates a positive sentiment (compound score >= 0.05), the customer is considered happy; otherwise, they are marked as unhappy.
* The existing is_happy_customer field is compared with the calculated sentiment score. If they don’t match, the is_happy_customer value is updated to align with the review sentiment.
* Special cases, such as neutral reviews or reviews where the sentiment is unclear, are skipped, ensuring only relevant changes are made.
After all corrections, the code tracks modifications by updating the modified_row column, which records the rows that have been changed. This ensures transparency and accountability in the data cleaning process.

#### 2.6.3 Implemented Code
"""

# Create the sentiment analyzer instance
sentiment_analyzer = SentimentIntensityAnalyzer()

# Function to preprocess the review text by removing unwanted characters
def preprocess_review(text):
    # Remove non-alphanumeric characters except spaces and convert to lowercase
    text = re.sub(r'[^a-z0-9\s]', '', text.lower())

    return text

# Function to validate if the sentiment matches the 'is_happy_customer' field
def validate_sentiment(review_text, reported_happiness):
    """Check if the review sentiment aligns with the reported happiness status."""

    # Preprocess the review text
    cleaned_text = preprocess_review(review_text)

    # Analyze the sentiment using VADER
    sentiment_score = sentiment_analyzer.polarity_scores(cleaned_text)
    compound_score = sentiment_score['compound']

    # Determine predicted happiness based on compound score
    predicted_happiness = compound_score >= 0.05  # True if positive, False otherwise

    # Check if predicted happiness matches reported happiness
    if pd.isna(reported_happiness) or predicted_happiness != reported_happiness:
        return False  # Mismatch found
    return True  # Sentiment matches reported happines

# Collect indices of rows with mismatched sentiment and reported happiness
mismatched_indices = []

for index, row in dirty_data.iterrows():
    review_text = row['latest_customer_review']

    if pd.notna(review_text):  # Process only non-null reviews
        is_matching = validate_sentiment(review_text, row['is_happy_customer'])

        if not is_matching:
            mismatched_indices.append(index)

print(mismatched_indices)

len(mismatched_indices)

dirty_data.loc[mismatched_indices]

dirty_data.loc[14]['latest_customer_review']

"""It more likely a neutral statement. And since the row is already modified. We are going to skip that row from modifying"""

# Iterate over mismatched rows and update the 'is_happy_customer' field
for index in mismatched_indices:
    if index == 14:
        continue  # Skip the row with index 14

    # Get the review text
    review_text = dirty_data.at[index, 'latest_customer_review']

    # Preprocess the review text
    cleaned_review = preprocess_review(review_text)

    # Get the sentiment polarity score using VADER
    sentiment_score = sentiment_analyzer.polarity_scores(cleaned_review)

    # Update the 'is_happy_customer' field based on the compound score
    dirty_data.at[index, 'is_happy_customer'] = sentiment_score['compound'] >= 0.05

    # Mark the row as modified by setting the 'modified_row' column to 1
    dirty_data.at[index, 'modified_row'] = 1

dirty_data['modified_row'].describe()

dirty_data['modified_row'].sum()

"""#### Now we need to check and clean the is_expedited_delivery column since it is not error free.
- But for that we need to train the regression models, since it is one of the features for regression model (as given in the specification) and delivery_charges are error free.
- We can use the model to predict where the prediction is wrong due to misleading values in is_expedited_delivery.
<br>
- And for creating the model we will need more that which we can get after filling the missing data

# 3. Filling up Missing values in the Missing Data

### 3.1 Understanding Missing data Structure.
"""

missing_data.info()

"""#### 3.1.2 Methodology to find missing values.

- For nearest_warehouse: Use customer_lat and customer_long and we already have geolocations for the warehouse which are stored in warehouse_data dataframe. Use the existing haversine() to calculate the distance and above functions to fill these missing values.
- Similary those functions will also calculate the distance so use that to fill distance_to_nearest_warehouse.
- For order_price, we have unit prices stored in unit_prices_dict. Use that and shopping_cart to impute the order_price column.
- For order_total, use this formula order_price * (1 - row['coupon_discount'] / 100) + row['delivery_charges'] and impute the values.
- For is_happy_customer, we can use the existing text preprocessing function and SentimentintensityAnalyzer() on latest_customer_review column to predict the is_happy_customer column

### 3.2 Calculating missing values for nearest_warehouse and distance_to_nearest_warehouse

#### 3.2.1 Implemented Methodlogy

* Existing Functions Reuse:
The code leverages the pre-defined find_nearest_warehouse() and haversine() functions to fill in missing values for the columns nearest_warehouse and distance_to_nearest_warehouse.

* Missing Data Identification:
A function fill_missing_warehouse_data() is created, which checks if either the nearest_warehouse or distance_to_nearest_warehouse values are missing (NaN) for each row.
If any missing value is found in these columns, the function calculates the nearest warehouse and the distance to that warehouse using the find_nearest_warehouse() function, which internally applies the Haversine formula to compute distances between geographic coordinates.

* Filling Missing Values:
For rows where the nearest_warehouse or distance_to_nearest_warehouse is missing, the function fills these values with the calculated nearest warehouse and the corresponding distance.

* Application to Dataset:
The function fill_missing_warehouse_data() is applied across the entire missing_data DataFrame to fill all missing values in the specified columns.
After applying the function, the missing data is checked to confirm that no NaN values remain in the nearest_warehouse and distance_to_nearest_warehouse columns.

* Validation:
The code validates that the missing data has been successfully filled by checking the summary statistics of the distance_to_nearest_warehouse column and the distribution of warehouse values in nearest_warehouse.
The value_counts() method is used to verify that the nearest_warehouse field has a balanced distribution across all available warehouse options, ensuring that the warehouse assignments are consistent.

#### 3.2.2 Implemendted Code
"""

# Function to fill missing 'nearest_warehouse' and 'distance_to_nearest_warehouse' values
def fill_missing_warehouse_data(row):
    # Check if 'nearest_warehouse' is missing
    if pd.isna(row['nearest_warehouse']):
        nearest_warehouse, _ = find_nearest_warehouse(row)
        row['nearest_warehouse'] = nearest_warehouse

    # Check if 'distance_to_nearest_warehouse' is missing
    if pd.isna(row['distance_to_nearest_warehouse']):
        _, distance = find_nearest_warehouse(row)
        row['distance_to_nearest_warehouse'] = round(distance, 4)  # Round distance to 4 decimal points

    return row

# Apply the function to fill missing values
missing_data = missing_data.apply(fill_missing_warehouse_data, axis=1)

# Verify the filled data
print(missing_data[['nearest_warehouse', 'distance_to_nearest_warehouse']].isna().sum())

# Check for the filled data in distance_to_nearest_warehouse
missing_data['distance_to_nearest_warehouse'].describe()

# Check for the filled data in nearest_warehouse
missing_data['nearest_warehouse'].value_counts()

missing_data.info()

"""### 3.3 Calculating missing values for order_price

#### 3.3.1 Implemented Methodology

* Utilizing Non-null shopping_cart:
Since the shopping_cart field is non-null for all rows, it can be used to accurately calculate the order_price for any rows where this value is missing.
Each row's shopping_cart contains the items and their quantities, which are used to compute the total order price.
* Function to Fill Missing order_price:
A function fill_missing_order_price() is created to handle missing values in the order_price column.
For rows where the order_price is missing (NaN), the function parses the shopping_cart (which is stored as a string) into a dictionary format using ast.literal_eval().
* Price Calculation:
The function then calculates the total price of the order by multiplying each item's unit price (from a predefined dictionary unit_prices_dict) by the quantity of that item in the cart.
The total order price is computed by summing the prices of all items in the cart.
* Assigning the Calculated Price:
Once the total price is calculated, it is assigned to the order_price field for that row.
* Applying the Function:
The fill_missing_order_price() function is applied to the entire missing_data DataFrame, filling in all missing order_price values based on the shopping cart contents.
* Validation:
The code checks the order_price column to ensure there are no more missing (NaN) values after the function has been applied.

#### 3.3.2 Implemented Code
"""

# Function to fill missing order_price values
def fill_missing_order_price(row):
    if pd.isna(row['order_price']):
        # Parse the shopping cart from the string format
        shopping_cart = ast.literal_eval(row['shopping_cart'])

        # Calculate the total price of the shopping cart
        total_price = sum(unit_prices_dict[item] * quantity for item, quantity in shopping_cart)

        # Assign the calculated total price to the 'order_price' column
        row['order_price'] = total_price

    return row

# Apply the function to fill missing values for order_price
missing_data = missing_data.apply(fill_missing_order_price, axis=1)

# Verify the filled data
print(missing_data['order_price'].isna().sum())

missing_data['order_price'].describe()

"""### 3.4 Calculatig Missing Values for order_total.

#### 3.4.1 Implemented Methodology

* Formula for Calculating order_total:
The order_total is calculated using the formula:
order_total = order_price * (1 - coupon_discount / 100) + delivery_charges
This formula accounts for the order_price, applies any coupon_discount to reduce the price, and adds the delivery_charges to compute the total.
* Function to Fill Missing order_total:
A function fill_missing_order_total() is implemented to handle missing values in the order_totalcolumn.
The function first checks whether the required fields (order_price and delivery_charges) are present. If either value is missing (NaN), the function skips the calculation and leaves order_total as NaN.
If all necessary fields are available, the function computes the order_total using the formula and assigns the result to the order_total field.
* Handling Edge Cases:
If any required fields are missing, such as order_price or delivery_charges, the function keeps the order_total as NaN to avoid inaccurate calculations.
* Applying the Function:
The fill_missing_order_total() function is applied to the entire missing_data DataFrame to fill in all missing values for the order_total column.
* Validation:
After applying the function, the code verifies that all missing values in the order_total column have been filled. A check is performed using isna().sum() to ensure there are no remaining missing values.

#### 3.4.2 Implemented Code
"""

# Function to fill missing order_total values
def fill_missing_order_total(row):
    # Check if any required fields are missing
    if pd.isna(row['order_price']) or pd.isna(row['delivery_charges']):
        # If any required value is NaN, keep 'order_total' as NaN
        return row

    # If all required fields are present, calculate the order_total
    row['order_total'] = row['order_price'] * (1 - row['coupon_discount'] / 100) + row['delivery_charges']

    return row

# Apply the function to fill missing values for order_total
missing_data = missing_data.apply(fill_missing_order_total, axis=1)

# Verify the filled data
print(missing_data['order_total'].isna().sum())

missing_data['order_total'].describe()

"""### 3.5 Calculating Missing Values for is_happy_customer

#### 3.5.1 Implemented Methodlogy:

* Sentiment Analysis Approach:
The code utilizes sentiment analysis to predict whether a customer is happy (is_happy_customer) based on the sentiment of their review. A compound sentiment score is calculated for each review using the VADER SentimentIntensityAnalyzer.
* Prediction Process:
The review text is first cleaned using preprocess_review() to remove unnecessary characters. Then, the sentiment score is computed, and based on the score, the is_happy_customer value is set to True (if the score is positive) or False (if negative).
* Applying the Prediction:
The function is applied to all rows in the dataset, filling in any missing values for is_happy_customer by analyzing the corresponding review sentiment.
* Validation:
After applying the function, the code checks to ensure that no missing values remain in the is_happy_customer field and confirms that the distribution of happy and unhappy customers is reasonable.

#### 3.5.2 Implemented Code
"""

missing_data['is_happy_customer'].value_counts()

# Function to get the value of is_happy_customer column based on sentiment intensity analysis
def predict_is_happy_customer(row):
    # Check if 'is_happy_customer' is missing
    if pd.isna(row['is_happy_customer']):
        # Preprocess the review text
        cleaned_review = preprocess_review(row['latest_customer_review'])

        # Analyze the sentiment using VADER
        sentiment_score = sentiment_analyzer.polarity_scores(cleaned_review)
        compound_score = sentiment_score['compound']

        # Predict 'is_happy_customer' as 1 for positive sentiment or 0 otherwise
        row['is_happy_customer'] = 1 if compound_score >= 0.05 else 0

    return row

# Apply the function to all rows to predict is_happy_customer
missing_data = missing_data.apply(predict_is_happy_customer, axis=1)

# Verify the filled data
print(missing_data['is_happy_customer'].isna().sum())

missing_data['is_happy_customer'].value_counts()

missing_data.info()

"""### 3.6 Predicting missing values for delivery_charges.

#### 3.6.1 Implemented Methodlogy

Methodlogy Imlemented to predict missing values: In the requriment, it is given that, delivery charge is calculated using a linear model which differs depending on the season. Also it is clearly mentioned that, **The model depends linearly (but in different ways for each season)**.
- And it depends on the following features:
  - Distance between customer and nearest warehouse
  - Whether the customer wants an expedited delivery
  - Whether the customer was happy with his/her last purchase (if no previous purchase,
it is assumed that the customer is happy)
- So this means, we need to build four separate linear regression models for each season (like one for summer, one for winter, etc.) with season-specific coefficients for factors like distance, expedited delivery, and customer satisfaction to predict delivery_charges.
- Here we will need more possible data. So we are going to use the cleaned data from the dirty_data (rows which are already modified) and missing_data where we don't have null delivery_charges.

#### 3.6.2 Implemented Code
"""

# Filter rows from dirty_data where modified_row == 1l
filtered_dirty_data = dirty_data[(dirty_data['modified_row'] == 1)]

filtered_dirty_data

# Combine filtered dirty_data and missing_data where delivery_charges is not null
merged_data = pd.concat([
    filtered_dirty_data,
    missing_data[missing_data['delivery_charges'].notna()]  # Rows from missing_data where delivery_charges is not null
])

merged_data

merged_data['distance_to_nearest_warehouse'].describe()

# Function to prepare features and target for a specific season
def prepare_season_data(season, data):
    """
    This function prepares the feature set (X) and target (y) for a specific season.
    """
    season_data = data[data['season'] == season]  # Filter data for the given season

    # Features: distance_to_nearest_warehouse, is_expedited_delivery, is_happy_customer
    X = season_data[['distance_to_nearest_warehouse', 'is_expedited_delivery', 'is_happy_customer']]

    # Target: delivery_charges
    y = season_data['delivery_charges']

    return X, y

# Train a linear regression model for each season and calculate R²
season_models = {}
season_r2_scores_test = {}
seasons = ['Spring', 'Summer', 'Autumn', 'Winter']

for season in seasons:
    # Prepare the data for the season
    X, y = prepare_season_data(season, merged_data)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Initialize and train the linear regression model
    model = LinearRegression()
    model.fit(X_train, y_train)

    # Store the model for the season
    season_models[season] = model

    # Predict on the test set
    y_pred_test = model.predict(X_test)

    # Calculate and store the R² value for the test data
    r2_test = r2_score(y_test, y_pred_test)
    season_r2_scores_test[season] = r2_test

    # Print R² value for the current season on test data
    print(f"R² value for {season} model on test data: {r2_test:.4f}")

# Display the four models
season_models

# Display the missing values to be filled
missing_data.loc[missing_data['delivery_charges'].isna(), 'delivery_charges']

# List to track the indices of the rows where delivery_charges were filled
filled_indices = []

def predict_delivery_charges(row):
    # Extract the features for the row and make them into a DataFrame with correct column names
    X = pd.DataFrame([[row['distance_to_nearest_warehouse'],
                       row['is_expedited_delivery'],
                       row['is_happy_customer']]],
                     columns=['distance_to_nearest_warehouse', 'is_expedited_delivery', 'is_happy_customer'])

    # Predict delivery charges using the model for the corresponding season
    model = season_models.get(row['season'])

    # Predict the value
    predicted_value = round(model.predict(X)[0], 2)    # Rounding it off till 2 decimal points since the dataframe is consistent with this format

    # Track the index of the row where we are filling the missing value
    filled_indices.append(row.name)

    # Return the predicted value
    return predicted_value

missing_data.loc[missing_data['delivery_charges'].isna(), 'delivery_charges'] = \
    missing_data[missing_data['delivery_charges'].isna()].apply(predict_delivery_charges, axis=1)

# Print the rows in missing_data for the given indices
missing_data.loc[filled_indices]

# Check the null values and the datatypes again
missing_data.info()

"""The missing data is complete now!

### 4. Predicting values for is_expedited_delivery using the models

#### 4.1 Identifying the anomalies
"""

# Apply the prediction function to all rows in dirty_data and store the results in a new column 'temp_delivery_charges'
dirty_data['temp_delivery_charges'] = dirty_data.apply(predict_delivery_charges, axis=1)

# Check the first few rows to verify the new column
dirty_data[['delivery_charges','temp_delivery_charges']].head(10)

"""#### 4.2 Implemented Methodology

* Initial Prediction and Residual Calculation:
The code starts by predicting delivery charges using a model and comparing these predictions with the actual delivery charges. The difference between predicted and actual charges, called the residual, is calculated for each row. This residual helps identify rows where the predicted values deviate significantly from the actual charges.
* Identifying Outliers:
A box plot of the residuals is created to visualize outliers. The Interquartile Range (IQR) method is used to determine the upper and lower bounds for outliers. Rows with residuals outside these bounds are flagged as outliers, indicating potential issues with the data.
* Flipping is_expedited_delivery:
The hypothesis is that errors in the is_expedited_delivery field might be causing these outliers. To test this, the code flips the value of is_expedited_delivery (from True to False or vice versa) for the outlier rows and recalculates the delivery charges with this flipped value.
* Recalculating and Comparing Residuals:
After flipping the is_expedited_delivery value, new delivery charges are predicted, and the flipped residuals (difference between the new predicted and actual charges) are calculated. These new residuals are compared to the original residuals to assess whether flipping the value improved the prediction accuracy.
* Improved Accuracy:
The box plot of the flipped residuals shows that the residuals are now more centralized around zero, indicating a significant reduction in errors. This confirms that the original is_expedited_delivery values were likely incorrect, causing large deviations in the predictions.
* Final Correction:
Based on the results, the is_expedited_delivery values are permanently flipped for the outlier rows, and these rows are marked as modified by incrementing the modified_row column.

#### 4.3 Implemented Code
"""

# Calculate the residual (difference between predicted and original delivery charges)
dirty_data['residual'] = dirty_data['temp_delivery_charges'] - dirty_data['delivery_charges']

# Check the first few rows to verify the new column
print(dirty_data[['delivery_charges', 'temp_delivery_charges', 'residual']].head())

"""##### 4.3.1. Plot the box plot of the residual
- This will give us an idea about which row of is_expedited_delivery contains errors
"""

import matplotlib.pyplot as plt

# Create a box plot of the residuals
plt.figure(figsize=(8, 6))
plt.boxplot(dirty_data['residual'].dropna())  # Drop NaN values if any in residuals
plt.title('Box Plot of Residuals')
plt.ylabel('Residual')
plt.grid(True)
plt.show()

"""##### 4.3.2. Using the quartile ranges show the outliers"""

# Calculate Q1 (25th percentile) and Q3 (75th percentile) for the residuals
Q1 = dirty_data['residual'].quantile(0.25)
Q3 = dirty_data['residual'].quantile(0.75)
IQR = Q3 - Q1  # Interquartile range

# Define the bounds for outliers
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Get the outliers in residuals
outliers = dirty_data[(dirty_data['residual'] < lower_bound) | (dirty_data['residual'] > upper_bound)]

# Print out the indices and values of the outliers
print("Outlier indices and their corresponding residuals:")
print(outliers[['residual']])

len(outliers[['residual']])

# Get the indices of the outliers
outlier_indices = dirty_data[(dirty_data['residual'] < lower_bound) | (dirty_data['residual'] > upper_bound)].index.tolist()

"""Check whether the index is already modified or not."""

modified_outlier_indices = []

for idx in outlier_indices:
    if dirty_data.loc[idx, 'modified_row'] == 1:
        print(f"Index {idx} is an outlier and has modified_row = 1")
        modified_outlier_indices.append(idx)

# Output the result lists
print("Outlier indices:", outlier_indices)
print("Modified outlier indices:", modified_outlier_indices)

dirty_data.loc[outlier_indices][['delivery_charges','temp_delivery_charges','residual','modified_row']]

"""So wherever we have outliers of the residual there we don't have any modified rows. Now we have to further check if we flip the value of is_expedited_delivery will it change the prediction and will it be closer.

##### 4.3.3. Check the residual after flipping the values of is_expedited_delivery
"""

# Function to predict delivery charges with flipped is_expedited_delivery
def predict_with_flipped_delivery(row):
    """
    Predict delivery charges with is_expedited_delivery flipped.
    """
    # Flip the value of is_expedited_delivery
    flipped_value = not row['is_expedited_delivery']

    # Create the features with the flipped value
    X_flipped = pd.DataFrame([[row['distance_to_nearest_warehouse'],
                               flipped_value,
                               row['is_happy_customer']]],
                             columns=['distance_to_nearest_warehouse', 'is_expedited_delivery', 'is_happy_customer'])

    # Predict delivery charges using the model for the corresponding season
    model = season_models.get(row['season'])
    new_predicted_value = model.predict(X_flipped)[0]

    return new_predicted_value

# Create a new column 'flipped_delivery_charges' for the outlier indices, keep the same value for others
dirty_data['flipped_delivery_charges'] = dirty_data.apply(
    lambda row: predict_with_flipped_delivery(row) if row.name in outlier_indices else row['temp_delivery_charges'],
    axis=1
)

# Calculate the residuals for the new predictions after flipping for outliers, keep the same for non-outliers
dirty_data['flipped_residual'] = dirty_data.apply(
    lambda row: row['flipped_delivery_charges'] - row['delivery_charges'] if row.name in outlier_indices else row['residual'],
    axis=1
)

# Display the relevant columns to verify the results
print(dirty_data[['delivery_charges', 'temp_delivery_charges','residual', 'flipped_delivery_charges', 'flipped_residual']].head())

dirty_data.loc[outlier_indices][['delivery_charges', 'temp_delivery_charges','residual', 'flipped_delivery_charges', 'flipped_residual']]

"""Plot the box plot of residuals after flipping the values in is_expedited_delivery"""

import matplotlib.pyplot as plt

# Create a box plot for the flipped_residuals
plt.figure(figsize=(8, 6))
plt.boxplot(dirty_data['flipped_residual'].dropna())  # Drop NaN values if any in flipped_residual
plt.title('Box Plot of Flipped Residuals')
plt.ylabel('Flipped Residual')
plt.grid(True)
plt.show()

"""- Based on the analysis of the box plots before and after flipping the is_expedited_delivery values for the outlier rows, it’s clear that the outliers in these rows were likely due to errors in the is_expedited_delivery column.
- Flipping these values significantly improved the accuracy of the predictions by reducing the residual. Initially, the residuals showed a wide spread with many extreme outliers, indicating that the model’s predictions deviated considerably from the actual delivery charges. After flipping is_expedited_delivery, the residuals became much more centralized around zero, and the number of outliers drastically reduced.
- This suggests that the incorrect is_expedited_delivery values were causing the large deviations in predictions. And we need to flip these values.
"""

# Verify the changes by checking the outlier rows before flipping
dirty_data.loc[outlier_indices, ['is_expedited_delivery', 'residual']].head()

# Flip the is_expedited_delivery values for the rows with outliers
dirty_data.loc[outlier_indices, 'is_expedited_delivery'] = dirty_data.loc[outlier_indices, 'is_expedited_delivery'].apply(lambda x: not x)

# After flipping, add +1 to the modified_row column for the rows in outlier_indices
dirty_data.loc[outlier_indices, 'modified_row'] += 1

# Verify the changes by checking the outlier rows after flipping the values
dirty_data.loc[outlier_indices, ['is_expedited_delivery', 'flipped_residual']].head()

dirty_data['modified_row'].sum()

"""So we have rectified 297 errors in the dirty data and we have corrected those errors.

# 5. Rectifying Outliers in the Outlier Data

* Here we need to detect and remove outlier rows in delivery_charges only

### 5.1 Understanding Structure of the Outlier Data Set.
"""

# Check for null values in outlier_data if any
outlier_data.info()

"""### 5.2 Implemented Methodlogy

To accurately identify outliers in delivery_charges, we cannot rely on simple methods like box plots or scatter plots. This is because delivery_charges are influenced by multiple factors (multivariate dependencies), including the distance to the nearest warehouse, whether the delivery is expedited, and customer satisfaction.
- These factors vary depending on the season, and they all play a role in determining the expected delivery charge. **If we try to identify outliers without considering these relationships, we might miss important patterns or incorrectly label normal data points as outliers.**

- By using models that take these key attributes into account, we can predict what the delivery charges should be for each order. The difference between the actual delivery charge and the predicted value, known as the residual, helps us understand how much each charge deviates from the expected value.
- Plotting (Box plot) these residuals gives us a clearer view of which delivery charges are unusually high or low, relative to the influencing factors.

### 5.3 Implemented Code

##### Now to create a new column named residual in your outlier_data DataFrame, we can use the previously trained models for each season to predict the delivery_charges and compute the residuals as the difference between the original delivery_charges and the predicted values.

##### 5.3.1 Predict the values of delivery_charges
"""

# Function to predict delivery charges for each row in outlier_data using the trained models
def predict_delivery_charges_outlier(row):
    # Extract the relevant features for prediction
    X = pd.DataFrame([[row['distance_to_nearest_warehouse'],
                       row['is_expedited_delivery'],
                       row['is_happy_customer']]],
                     columns=['distance_to_nearest_warehouse', 'is_expedited_delivery', 'is_happy_customer'])

    # Get the model based on the season for this row
    model = season_models.get(row['season'])

    # Predict the delivery charges for this row
    return round(model.predict(X)[0], 2)

# Create a new column 'predicted_delivery_charges' using the models
outlier_data['predicted_delivery_charges'] = outlier_data.apply(predict_delivery_charges_outlier, axis=1)

outlier_data[['delivery_charges', 'predicted_delivery_charges']]

"""##### 5.3.2 Finding and Plotting the outliers"""

# Calculate the residuals as the difference between original and predicted delivery charges
outlier_data['residual'] = outlier_data['delivery_charges'] - outlier_data['predicted_delivery_charges']

# Display the residuals along with original delivery charges and predicted delivery charges
outlier_data[['delivery_charges', 'predicted_delivery_charges', 'residual']]

"""Now plotting a box plot of the residuals"""

# Plot a box plot of the residuals to identify potential outliers
plt.figure(figsize=(8, 6))
plt.boxplot(outlier_data['residual'].dropna())  # Ensure to drop NaN values
plt.title('Box Plot of Residuals for Delivery Charges')
plt.ylabel('Residual')
plt.grid(True)
plt.show()

"""##### Filtering out the outliers using quartlie method.
- Here we are considering the lower bound and upper bound of the box plot to remove the outliers. And instead of using the delivery_charges right away we are using the residual which we have get from the delivery_charges and predicted delivery_charges.
"""

# Calculate the interquartile range (IQR) to identify outliers
Q1 = outlier_data['residual'].quantile(0.25)
Q3 = outlier_data['residual'].quantile(0.75)
IQR = Q3 - Q1

# Define outlier boundaries
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Get the outliers based on the residuals
outliers = outlier_data[(outlier_data['residual'] <= lower_bound) | (outlier_data['residual'] >= upper_bound)]

outliers[['delivery_charges', 'predicted_delivery_charges', 'residual']]

# Get the non-outliers based on the residuals
without_outliers = outlier_data[(outlier_data['residual'] >= lower_bound) & (outlier_data['residual'] <= upper_bound)]

without_outliers[['delivery_charges', 'predicted_delivery_charges', 'residual']]

# Plot a box plot of the residuals after removing the outliers
plt.figure(figsize=(8, 6))
plt.boxplot(without_outliers['residual'].dropna())  # Ensure to drop NaN values
plt.title('Box Plot of Residuals for Delivery Charges')
plt.ylabel('Residual')
plt.grid(True)
plt.show()

"""# 6. Save the cleaned and modified solution csv

### 6.1 Saving the outlier solution file
"""

without_outliers.columns

# Drop the predicted_delivery_charges and residual columns from the outlier_data
without_outliers = without_outliers.drop(columns=['predicted_delivery_charges', 'residual'])

# Verify by showing the first few rows
without_outliers.head()

without_outliers.shape

"""There were 21 outliers in the outlier data which we have found and we have successfully removed them.

Check the dtypes and columns before saving the file
"""

without_outliers.columns

without_outliers.info()

without_outliers.to_csv("Group142_outlier_data_solution.csv", index = False)

"""### 6.2 Saving the Dirty data solution file"""

dirty_data.shape

dirty_data.columns

dirty_data['modified_row'].sum()

# Drop the specified columns from the dirty_data DataFrame
dirty_data = dirty_data.drop(columns=['modified_row', 'calculated_nearest_warehouse', 'calculated_distance_to_warehouse',
                                      'temp_delivery_charges', 'residual', 'flipped_delivery_charges', 'flipped_residual'])

dirty_data.head()

dirty_data.shape

dirty_data.columns

dirty_data.info()

# Save the .csv
dirty_data.to_csv("Group142_dirty_data_solution.csv", index = False)

"""### 6.3 Saving the missing data solution file"""

# Check for columns in missing data
missing_data.columns

missing_data.shape

# Displaying and Validating first few rows
missing_data.head()

# Checking for missing values
missing_data.info()

# Saving the solution csv
missing_data.to_csv("Group142_missing_data_solution.csv", index = False)

"""# 7. References:
- https://help.arcgis.com/en/geodatabase/10.0/sdk/arcsde/concepts/geometry/coordref/coordsys/geographic/geographic.htm#
- https://realpython.com/python-linear-algebra/
- https://medium.com/@gelsonm/to-use-or-lose-stop-words-in-nlp-de946edaa468
"""

