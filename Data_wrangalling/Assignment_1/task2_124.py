# -*- coding: utf-8 -*-
"""task2_124.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nkjgsiHDdlTXCRkCYdYC6xUIVU3Vlahm

<div class="alert alert-block alert-success">
    
# FIT5196 Task 2: Text Pre-Processing
#### Group_Number: 124
#### Student Name: Pankaj Shitole
#### Student ID: 33570523
#### Student Name: Sachin Shivaramaiah
#### Student ID: 34194037


Date: 30/08/2024


Environment: Google Colab

---

</div>

## Install the required libraries
Install the libraries mentioned below to run the script
"""

# !pip install emoji
# !pip install nltk

"""## Import the required libraries"""

import pandas as pd
import json    # To parse the json file from task 1
import os
import re    # Regular expression
import emoji    # To remove the remaining emojis
import nltk    # Text processing
from nltk.stem import PorterStemmer
from collections import defaultdict, Counter
from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures
from nltk.util import ngrams
from __future__ import division
from itertools import chain
from nltk.probability import FreqDist
from nltk.probability import *
from nltk.tokenize import RegexpTokenizer

"""## Step 1: Text extraction
*   You are required to extract the review text for the gmap ids which has at least 70 reviews from the output of task 1


"""

# Load the Task 1 CSV file
csv_df = pd.read_csv('task1_124.csv', encoding='utf-8')

# Filter the gmap_id where review_text_count is greater than or equal to 70
filtered_df = csv_df[csv_df['review_text_count'] >= 70]

# Store the extracted gmap ids
filtered_gmap_ids = filtered_df['gmap_id'].tolist()

len(filtered_gmap_ids)

"""Load the json file since it contains all the reviews. Now we have gmapids with reviews more than 70, we can use that list to fetch the data from the json file."""

# Load the Task 1 JSON file
with open('task1_124.json', 'r', encoding='utf-8') as file:
    json_data = json.load(file)

# Create a list to store the extracted reviews
extracted_reviews = []

# Iterate over the gmap_ids to extract their review_text
for gmap_id in filtered_gmap_ids:
    if gmap_id in json_data:
        for review in json_data[gmap_id]['reviews']:
            extracted_reviews.append({
                'gmap_id': gmap_id,
                'review_text': review['review_text']
            })

"""Create a dataframe from the extracted reviews"""

# Convert the list to a DataFrame
reviews_df = pd.DataFrame(extracted_reviews)

# Remove rows where review_text is None
reviews_df = reviews_df.dropna(subset=['review_text'])
reviews_df = reviews_df[reviews_df['review_text'].str.strip().str.lower() != 'none']  # Remove rows where review_text is "None"

reviews_df

"""In Task 1, we had some empty reviews because we have removed the emojis from the reviews. Here in task 2, we have to deal with text data and these empty strings doesn't make any sense."""

reviews_df[reviews_df['review_text'] == '']

reviews_df = reviews_df[reviews_df['review_text'] != ""]     # Remove the reviews with empty strings

"""### Convert all the reviews to lower case"""

reviews_df.loc[:,'review_text'] = reviews_df['review_text'].apply(lambda x: x.lower() if isinstance(x, str) else x)

"""### Remove all the emojis"""

# Function to remove emojis using the emoji library
def remove_emojis(text):
    return emoji.replace_emoji(text, replace='')

reviews_df.loc[:,'review_text_cleaned'] = reviews_df['review_text'].apply(remove_emojis)    # Remove all the emojis from the text. Reference [1]

reviews_df['review_text_cleaned'].value_counts()

# Again removing the reviews which are empty strings after removing the emojis
reviews_df = reviews_df[reviews_df['review_text_cleaned'] != ""]

reviews_df

"""## Step 2: Generate the unigram and bigram lists and output as vocab.txt

### Step 2.1. Removing non alphabetic characters
"""

tokenizer = RegexpTokenizer(r"[a-zA-Z]+")

def tokenize_reviews_by_gmap_id(reviews_df):
    gmap_unigrams = defaultdict(list)        # To store unigrams per gmap_id
    gmap_bigrams_dict = defaultdict(list)  # To store bigrams per gmap_id

    for index, row in reviews_df.iterrows():
        gmap_id = row['gmap_id']
        review_text = row['review_text']

        # Tokenize the review text into unigrams
        unigram_tokens = tokenizer.tokenize(review_text)

        # Store the unigrams for this review under the respective gmap_id
        gmap_unigrams[gmap_id].extend(unigram_tokens)

        # Extract bigrams within the same review, so itâ€™s easy to back track them to gmap_id
        bigram_tokens = list(ngrams(unigram_tokens, 2))  # Extract bigrams within the review

        # Store the bigrams for this review under the respective gmap_id
        gmap_bigrams_dict[gmap_id].extend(bigram_tokens)

    return gmap_unigrams, gmap_bigrams_dict

gmap_unigrams_dict, gmap_bigrams_dict = tokenize_reviews_by_gmap_id(reviews_df)

"""### Step 2.2. Generating First 200 meaningful bigrams

Let's create top 200 bigrams
"""

# Use bigram finder to collect the bigrams from unigrams
bigram_measures = BigramAssocMeasures()
bigram_finder = BigramCollocationFinder.from_words(list(gmap_unigrams_dict.values())[0])

# Fetch top 200 bigrams using pmi
top_200_bigrams = bigram_finder.nbest(bigram_measures.pmi, 200)
top_200_bigrams

# Convert top bigrams to a set for quick lookup
top_200_bigrams_set = set(top_200_bigrams)

"""No need to remove the tokens with length less than length 3 before combining them for bigrams because they might posses some information.
For example, let's consider two unigrams 'not' and 'necessary'. So if we remove the 'not' considering it as a stopword the complete meaning of the context will change. Instead of 'not necessary' it will be considered as necessary

We have to add the respective top 200 bigrams to their respective gmap_ids. So now using the the gmap_bigrams_dict to validate the legit bigrams
"""

# Add these meaningful bigrams (collocations) to the final vocabulary
final_vocab = defaultdict(list)
for gmap_id, unigrams in gmap_unigrams_dict.items():
    # Keep all unigrams in the final vocab
    final_vocab[gmap_id].extend(unigrams)

    # Only add bigrams that are in the top 200 meaningful bigrams
    for bigram in gmap_bigrams_dict[gmap_id]:
        if bigram in top_200_bigrams_set:
            # Convert the bigram tuple to a string and add to the final vocab
            final_vocab[gmap_id].append('_'.join(bigram))

# Display the head of the final vocabulary (first few entries)
final_vocab_head = dict(list(final_vocab.items())[:1])
print("Final Vocabulary with Unigrams and Meaningful Bigrams (Collocations):")
print(final_vocab_head)

"""Check for the lexical diversity and other statistical information of the reviews"""

words = list(chain.from_iterable(final_vocab.values()))
vocab = set(words)

# Calculate Lexical Diversity
lexical_diversity = len(words)/len(vocab)

print("---Vocab---")
print("Vocabulary size: ",len(vocab),"\nTotal number of tokens: ", len(words), \
"\nLexical diversity: ", lexical_diversity)

"""Let's check for term frequency for the tokens"""

fd_1 = FreqDist(words)
for i, word in enumerate(fd_1.most_common(25)):
    print(word, end = '\n' if (i+1) % 5 == 0 else ' ')

"""Here after careful observation and after considering the combined token set, we are finding the frequency of each token. From the resultant output in the above cell it is very clear that majority of the frequency are occupied by the stop words. The next step would be removing the stop words.

### Step 2.3. Removal of context-independent stop words

Mount the drive and read the stopwords file
"""

# Mount the drive for stopwords text file
from google.colab import drive
drive.mount('/content/drive')
folder_path = '/content/drive/Shareddrives/FIT5196_S2_2024/GroupAssessment1/'

# Load the provided stopwords from stopwords_en.txt
with open(f'{folder_path}stopwords_en.txt', 'r') as file:
    context_independent_stopwords = set(file.read().splitlines())

len(context_independent_stopwords)

"""Remove the context independent stopwords from the respective gmap_ids"""

for gmap_id in final_vocab:
    final_vocab[gmap_id] = [token for token in final_vocab[gmap_id] if token.lower() not in context_independent_stopwords]

words_1 = list(chain.from_iterable(final_vocab.values()))
vocab_1 = set(words_1)
# Calculate Lexical Diversity
lexical_diversity_1 = len(words_1)/len(vocab_1)
print("---Vocab after removing context-independent stop words---")
print("Vocabulary size: ",len(vocab_1),"\nTotal number of tokens: ", len(words_1), \
"\nLexical diversity: ", lexical_diversity_1)

"""### Step 2.4. Removal of context-dependent stop words"""

# Count the number of gmap_ids in which each word appears
word_gmap_count = Counter()

for tokens in final_vocab.values():
    unique_words = set(tokens)
    word_gmap_count.update(unique_words)

word_gmap_count

total_gmap = len(final_vocab)
threshold = 0.95 * total_gmap    # calculate the threshold
# Store the words which occur more than in 95% of total businesses
context_dependent_stopwords = {word for word, count in word_gmap_count.items() if count > threshold}

# remove the context-dependent stopwords from the final_vocab
for gmap_id in final_vocab:
    final_vocab[gmap_id] = [token for token in final_vocab[gmap_id] if token.lower() not in context_dependent_stopwords]

"""Let's print and see how vocab has changed after removing the context dependent stopwords"""

words_2 = list(chain.from_iterable(final_vocab.values()))
vocab_2 = set(words_2)
# Calculate Lexical Diversity
lexical_diversity_2 = len(words_2)/len(vocab_2)
print("---Vocab after removing context-dependent stop words---")
print("Vocabulary size: ",len(vocab_2),"\nTotal number of tokens: ", len(words_2), \
"\nLexical diversity: ", lexical_diversity_2)

"""<div class="alert alert-block alert-warning">
    
### Step 2.5. Remove the tokens with a length less than 3 from the vocab.  <a class="anchor" name="libs"></a>
 </div>

No need to worry about the bigrams because a bigram consists of two words. And each word should be of length 1. So x_y will be of length more than 2.
"""

# Remove tokens with a length less than 3 from the final_vocab
for gmap_id in final_vocab:
    final_vocab[gmap_id] = [token for token in final_vocab[gmap_id] if len(token) >= 3]

# Display the head of the final vocabulary (first few entries) after removing short tokens
final_vocab_head = dict(list(final_vocab.items())[:3])
print("Final Vocabulary after removing tokens with length less than 3:")
print(final_vocab_head)

words_3 = list(chain.from_iterable(final_vocab.values()))
vocab_3 = set(words_3)
# Calculate Lexical Diversity
lexical_diversity_3 = len(words_3)/len(vocab_3)
print("---Vocab after removing tokens with a length less than 3---")
print("Vocabulary size: ",len(vocab_3),"\nTotal number of tokens: ", len(words_3), \
"\nLexical diversity: ", lexical_diversity_3)

"""<div class="alert alert-block alert-warning">
    
### Step 2.6. Remove rare tokens.  <a class="anchor" name="libs"></a>
 </div>

Here some bigrams also comes into rare tokens. But we need to retain those bigrams
"""

# Count how many businesses each token appears in
token_business_count = defaultdict(set)

# Iterate over each `gmap_id` and track unique tokens in each business
for gmap_id in final_vocab:
    unique_tokens = set(final_vocab.get(gmap_id, []))  # Get unique tokens from the vocab
    for token in unique_tokens:
        token_business_count[token].add(gmap_id)  # Track in which businesses each token appears

# Calculate the threshold (5% of the total number of businesses in `final_vocab`)
threshold = 0.05 * len(final_vocab)    # Tokens which are in less than 5% of the business

# Identify rare tokens (tokens appearing in fewer businesses than the threshold)
rare_tokens = {token for token, businesses in token_business_count.items() if len(businesses) < threshold}
len(rare_tokens)

rare_bigrams = {token for token in rare_tokens if '_' in token}
len(rare_bigrams)

"""So we need to retain these bigrams for getting removed

"""

# Step to remove rare unigrams from final_vocab
for gmap_id in final_vocab:
    final_vocab[gmap_id] = [
        token for token in final_vocab[gmap_id]
        if (token not in rare_tokens) or ('_' in token)  # Keep bigrams
    ]

words_4 = list(chain.from_iterable(final_vocab.values()))
vocab_4 = set(words_4)
# Calculate Lexical Diversity
lexical_diversity_4 = len(words_4)/len(vocab_4)
print("---Vocab after removing rare tokens---")
print("Vocabulary size: ",len(vocab_4),"\nTotal number of tokens: ", len(words_4), \
"\nLexical diversity: ", lexical_diversity_4)

"""<div class="alert alert-block alert-warning">
    
### Step 2.7. Perform stemming on remaining tokens  <a class="anchor" name="libs"></a>
 </div>

Same like previous steps, there no sense in stemming the bigram tokens. So we are just going to stem the unigrams. Similar to the last steps we are using â€˜_â€™ to identify the bigrams and keeping them away from the stemming purpose
"""

stemmer = PorterStemmer()

skipped_bigrams = []

# Stem only unigrams in the final_vocab, and keep track of bigrams that are skipped
for gmap_id in final_vocab:
    updated_tokens = []
    for token in final_vocab[gmap_id]:
        if '_' in token:  # Check if the token is a bigram
            skipped_bigrams.append((gmap_id, token))  # Track the skipped bigram
            updated_tokens.append(token)  # Keep the bigram as it is
        else:
            updated_tokens.append(stemmer.stem(token))  # Stem unigrams

    final_vocab[gmap_id] = updated_tokens  # Update the vocab with the processed tokens

# Count unique bigrams which are skipped
unique_skipped_bigrams = set(bigram for gmap_id, bigram in skipped_bigrams)
num_unique_skipped_bigrams = len(unique_skipped_bigrams)

# Print the count of unique skipped bigrams
print(f"Number of unique bigrams that were skipped: {num_unique_skipped_bigrams}")

"""<div class="alert alert-block alert-warning">
    
### Step 2.8. Calculate the vocabulary containing both unigrams and bigrams  <a class="anchor" name="libs"></a>
 </div>
"""

words_5 = list(chain.from_iterable(final_vocab.values()))
vocab_5 = set(words_5)
# Calculate Lexical Diversity
lexical_diversity_5 = len(words_5)/len(vocab_5)
print("---Vocab after Stemming---")
print("Vocabulary size: ",len(vocab_5),"\nTotal number of tokens: ", len(words_5), \
"\nLexical diversity: ", lexical_diversity_5)

"""Here the vocabulary size is decreased a lot. We have 198 bigrams and remaining unigrams in the vocab.

<div class="alert alert-block alert-warning">
    
### Step 2.9. Create a vocab text file  <a class="anchor" name="libs"></a>
 </div>
"""

# Extract all unique tokens from final_vocab
unique_tokens = set()

for tokens in final_vocab.values():
    unique_tokens.update(tokens)

# Sort tokens alphabetically
sorted_tokens = sorted(unique_tokens)

# Write tokens to a file with their corresponding index
with open("124_vocab.txt", "w") as f:
    for index, token in enumerate(sorted_tokens):
        f.write(f"{token}:{index}\n")

# The vocab.txt file is now created with the required format
print("124_vocab.txt file has been created successfully!")

"""<div class="alert alert-block alert-warning">
    
## Step 3.  Generate the sparse numerical representation and output as countvec.txt <a class="anchor" name="libs"></a>
 </div>
"""

# Create a mapping of tokens to their indexes using the sorted_tokens list
token_to_index = {token: index for index, token in enumerate(sorted_tokens)}

# Count token frequencies for each gmap_id (i.e., for each document) using FreqDist and create the count_vec text file
with open("124_countvec.txt", "w") as f:
    for gmap_id, tokens in final_vocab.items():
        # Use FreqDist to count occurrences of tokens in this gmap_id
        token_counts = FreqDist(tokens)    # Document_frequency

        # Join the tokens' indices with their respective counts
        token_occurrence_str = ",".join(f"{token_to_index[token]}:{count}" for token, count in token_counts.items())

        # Write the gmap_id and token occurrences to the file
        f.write(f"{gmap_id}:{token_occurrence_str}\n")

print("Successfully created count_vec.txt file using FreqDist() function.")

"""## References:


*   [1] https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python

[Link to my workspace](https://colab.research.google.com/drive/1nkjgsiHDdlTXCRkCYdYC6xUIVU3Vlahm?usp=sharing)
"""

