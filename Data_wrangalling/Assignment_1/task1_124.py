# -*- coding: utf-8 -*-
"""task1_124.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bghOQ5NYKWxSBQdVG22qmqm98hHHSmS3

<div class="alert alert-block alert-success">
    
# FIT5196 Task 1 in Assessment 1
#### Group_Number: 124
#### Student Name: Pankaj Shitole
#### Student ID: 33570523
#### Student Name: Sachin Shivaramaiah
#### Student ID: 34194037


Date: 30/08/2024


Environment: google Colab

---




Libraries used:
* re (for regular expression, installed and imported)
* pandas (for data manipulation)
* datetime
* json
    
</div>

## 1. **INTRODUCTION**
Task 1 focuses on the essential first step of processing raw text data by extracting and structuring information from semi-structured Google Maps reviews. The task involves parsing various data formats, including mis-structured XML files and Excel sheets, to transform unorganized data into a structured format. The goal is to produce CSV and JSON files that summarize key metrics, such as review counts, text presence, and response times. This structured output will serve as the foundation for further analysis in subsequent tasks.

<div class="alert alert-block alert-warning">
    
## 2.  Importing Libraries  <a class="anchor" name="libs"></a>
 </div>

os: Helps in managing file operations, such as navigating directories and handling file paths, ensuring smooth access and organization of the input and output files.

pandas: Provides powerful tools for data manipulation and transformation, allowing us to efficiently process, clean, and analyze structured data, ultimately producing the required CSV outputs.

re: Enables pattern matching and text extraction from the semi-structured text files, which is essential for accurately parsing and processing raw review data.

datetime: Assists in handling and formatting date and time data, allowing us to standardize review timestamps and calculate metrics like the earliest and latest review dates.

json: Facilitates the creation and management of JSON outputs, ensuring that the extracted data is structured correctly for further analysis and storage
"""

# Import required libraries
import os
import pandas as pd
import re
from datetime import datetime
import json

"""Mount the drive"""

from google.colab import drive
drive.mount('/content/drive')

"""Folder path to access the files"""

folder_path = '/content/drive/Shareddrives/FIT5196_S2_2024/GroupAssessment1/Student Data/student_group124'

"""Change the current working directory"""

os.chdir(folder_path)

"""<div class="alert alert-block alert-warning">
    
## 3.  Loading and Reading Files <a class="anchor" name="libs"></a>
 </div>

**INCEPTION STEP:** The inception point would be to start from reading the file from the designated folder path which will be done with utf-8 encoding
"""

def read_txt_files(file_path):
    with open(f'{folder_path}/{file_path}', 'r', encoding = 'utf-8') as f:    # Passed folder path and file path
        return f.read()

"""Now iterate through the list of files in the current directory and store the string values of the text file in a list"""

text_files = []
excel_file = []

for file in os.listdir():    # list all files in the directory
    if file.endswith('.txt'):    # Check the file which is ending with .txt
        text_files.append(read_txt_files(file))
    else:
      excel_dictionary = pd.read_excel(file, engine = 'openpyxl', sheet_name = None)    # Reading multiple sheets in an excel file. Reference [1]
      # By default it is ready in dict() datatype
      # Converting it into list, since it's easy to parse and we are not required to store sheet names
      excel_file = list(excel_dictionary.values())

"""Check for number of files"""

len(text_files), len(excel_file)

"""<div class="alert alert-block alert-warning">
    
## 4. Regex-Based Pattern Matching and Storage  <a class="anchor" name="libs"></a>
 </div>

Storing all the patterns in the variables
"""

# Using (?i) to ignore the case. Reference provided
userid_pattern = r"(?i)<\s*user[_id.\s]*>\s*(.*?)\s*<\s*[/]*\s*user[_id.\s]*>"
gmapid_pattern = r"(?i)<\s*gmap[_\s]*id\s*>\s*(.*?)\s*<\s*/{1,2}\s*gmap[_\s]*id\s*>"
pics_pattern = r"(?i)<\s*(?:pics|pictures)\s*>\s*(.*?)\s*<\s*/{1,2}\s*(?:pics|pictures)\s*>"
review_pattern = r"(?i)<\s*(?:text|review)\s*>\s*(.*?)\s*<\s*/{1,2}\s*(?:text|review)\s*>"
rating_pattern = r"(?i)<\s*(?:rate|rating)\s*>\s*(.*?)\s*<\s*/{1,2}\s*(?:rate|rating)\s*>"
time_pattern = r"(?i)<\s*(?:time|date)\s*>\s*(.*?)\s*<\s*/{1,2}\s*(?:time|date)\s*>"
resp_pattern = r"(?i)<\s*(?:resp|response)\s*>\s*(.*?)\s*<\s*/{1,2}\s*(?:resp|response)\s*>"

patterns_list = [userid_pattern, resp_pattern, pics_pattern, review_pattern, rating_pattern, time_pattern, gmapid_pattern]

matches_count = []
for text in text_files:
  lst = []
  for patterns in patterns_list:
    matches = re.findall(patterns, text, re.DOTALL|re.IGNORECASE)    # Added the flags to ignore the case and to access the new line character using .*. Reference [2]
    lst.append(matches)
  matches_count.append(lst)

len(matches_count[0][0])

"""Check the number of tags extracted per file. It will give us an idea about consistency in fetching the tags"""

for number, file_ in enumerate(matches_count):
  print(f"File number: {number}")
  for tag in file_:
    print(len(tag))

# Initialize empty lists to store all matches for each pattern
all_user_ids = []
all_responses = []
all_pictures = []
all_reviews = []
all_ratings = []
all_times = []
all_gmap_ids = []

# List to store references to the lists above for easier management
all_matches_lists = [all_user_ids, all_responses, all_pictures, all_reviews, all_ratings, all_times, all_gmap_ids]

# Iterate over each match list (for each text file)
for match_lst in matches_count:
    for i, matches in enumerate(match_lst):
        # Extend the corresponding list with the matches found in this file
        all_matches_lists[i].extend(matches)

# Combine the data into a structure suitable for DataFrame
data = list(zip(*all_matches_lists))

# Create DataFrame with appropriate column names
df_text = pd.DataFrame(data, columns=['user_id', 'response', 'pic', 'reviews', 'ratings', 'time', 'gmapid'])

df_text

"""<div class="alert alert-block alert-warning">
    
## 5. Excel Data Parsing and Extraction  <a class="anchor" name="libs"></a>
 </div>
"""

#extracting the excel_data
col_names = ['user_id', 'resp', 'pics', 'text', 'rating', 'time', 'gmap_id']
filtered_sheets = [df[col_names] for df in excel_file]

#Concating all the filtered sheets into one sheet
df_excel = pd.concat(filtered_sheets, ignore_index=False)

#Printing the excel
df_excel

#Preprocessing the mereged data
df_excel.dropna(axis=0, how="all", inplace=True)    # drop the nas if any

# Rename the columns to make it consistent with the text files
df_excel.rename(columns={"resp": "response", "pics": "pic", "text": "reviews", "rating": "ratings", "gmap_id": "gmapid"}, inplace=True)

"""<div class="alert alert-block alert-warning">
    
## 6. Text and Excel Data Integration  <a class="anchor" name="libs"></a>
 </div>
"""

# Concatenate the text and excel dataframe
df = pd.concat([df_excel, df_text], ignore_index= True)

#Print after merging
df

"""<div class="alert alert-block alert-warning">
    
## 7. Data Pre-processing and Tranformation  <a class="anchor" name="libs"></a>
 </div>
"""

# Drop the duplicates if any
df.drop_duplicates(inplace=True)

# Check for response column for nan
df['response'].isna().sum()

df['response'].fillna("None", inplace = True)

# Convert the 'time' column to numeric
df['time'] = pd.to_numeric(df['time'], errors='coerce')

"""Create a function to convert timestamp to utc"""

def convert_timestamp_to_utc(timestamp_ms):
    # Convert milliseconds to seconds
    timestamp_s = timestamp_ms / 1000.0
    # Convert to datetime object in UTC
    dt_object = datetime.utcfromtimestamp(timestamp_s)
    # Format the datetime object to the desired format
    return dt_object.strftime('%Y-%m-%d %H:%M:%S')

df['time'] = df['time'].apply(convert_timestamp_to_utc)    # transform the time column in utc format

df['time'][0]

#Imputing None inplace of nan values
df["reviews"].fillna("None", inplace = True)

"""Extracting Eglish review translated by google. Steps below


"""

#Checking for the structure of the reviews specially for the translated to English ones.
indices = []
for i in range(len(df)):
     if "(Translated by Google)" in df.iloc[i, 3]:
         indices.append(i)
df.iloc[indices, 3]

#Extracting the review_text which were tranlated to English by google
def extract_english_reviews(reviews):
    if "(Translated by Google)" in reviews:
        pattern = r"\(Translated by Google\)\s*(.*?)\s*\(Original\)"
        matches = re.findall(pattern, reviews, re.DOTALL)
    else:
        return reviews
    return matches[0]

#Extraction
df.loc[:, 'reviews'] = df['reviews'].apply(lambda x: extract_english_reviews(x))

"""Extract the dimensions for the pic column"""

# Check for nan values in the column
df['pic'].isna().sum()

# Create a function to extract the pic dimension
dimension_pattern = re.compile(r'w(\d+)-h(\d+)')

# Function to extract dimensions from the 'pic' column
def extract_dimensions(pics_column):
    if pd.isna(pics_column) or pics_column in ["None", ""]:    # Considering all the None, nan and empty string if any
        return []
    else:
        # Find all matches of the pattern in the string
        dimensions = dimension_pattern.findall(pics_column)
        # Convert matches to list of lists (height, width)
        return [[str(h), str(w)] for w, h in dimensions]

# Apply the above function to the pic column and create a new dimensions column
df['dimensions'] = df['pic'].apply(extract_dimensions)

# Reorder the columns to place dimensions next to pics just for convenience
cols = df.columns.tolist()
pic_index = cols.index('pic')
cols.insert(pic_index + 1, cols.pop(cols.index('dimensions')))
df = df[cols]

"""<div class="alert alert-block alert-warning">
    
## 8. Data Serialization to JSON and CSV  <a class="anchor" name="libs"></a>
 </div>

Conversion to CSV file considering respective counts of the columns
"""

# create a new column to keep track of review text count
df.loc[:,'review_text_count'] = df['reviews'].apply(lambda x: 1 if x != "None" else 0)

#Response Count
df['response_count'] = df['response'].apply(lambda x: 1 if x != "None" else 0)

# Group by gmapid and aggregate the counts
result_df = df.groupby('gmapid').agg(
    review_count=('user_id', 'count'),  # Count the number of reviews
    review_text_count=('review_text_count', 'sum'),  # Sum the text review counts
    response_count=('response_count', 'sum')  # Sum the response counts
).reset_index()

# Rename the gmapid column to gmap_id for consistency with output format
result_df.rename(columns={'gmapid': 'gmap_id'}, inplace=True)

# Print the result_df
result_df

# Save the .csv file
result_df.to_csv('/content/task1_124.csv', index = False)
print("task1_124.csv successfully generated!")

"""Some pre-processing on reviews column"""

# Convert the review into lower case
df['reviews'] = df['reviews'].apply(lambda x: x.lower() if isinstance(x, str) and x != "None" else x)

# Remove the emojis
def remove_emojis(text):
    # Define a regex pattern for emojis
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"
        "\U0001F300-\U0001F5FF"
        "\U0001F680-\U0001F6FF"
        "\U0001F1E0-\U0001F1FF"
        "\U00002702-\U000027B0"
        "\U000024C2-\U0001F251"
        "]+",
        flags=re.UNICODE
    )    # Reference [3]
    # Remove emojis from the text
    return emoji_pattern.sub(r'', text)

# Apply the function to the 'reviews' column
df.loc[:,'reviews'] = df['reviews'].apply(lambda x: remove_emojis(x))

"""**JSON file formatting**"""

# Create a if_pic column. Place Y if there's a picture, N otherwise
df['if_pic'] = df['pic'].apply(lambda x: 'Y' if isinstance(x, str) and 'url' in x else 'N')

# Create a if_response column. Place Y if there's a response, N otherwise
df['if_response'] = df['response'].apply(lambda x: 'Y' if pd.notna(x) and x != 'None' else 'N')

# In the output we have ratings as float, so need to type cast it into float
# Convert review_rating to float
df.loc[:,'ratings'] = df['ratings'].apply(lambda x: float(x) if x != 'None' else x)

# Process the data into the desired JSON format
json_output = {}

for gmap_id, group in df.groupby('gmapid'):
    reviews = []
    for _, row in group.iterrows():
        reviews.append({
            'user_id': row['user_id'],
            'time': row['time'],
            'review_rating': row['ratings'],
            'review_text': row['reviews'],
            'if_pic': row['if_pic'],
            'pic_dim': row['dimensions'],  # Convert the string representation back to list of tuples
            'if_response': row['if_response']
        })

    json_output[gmap_id] = {
        'reviews': reviews,
        'earliest_review_date': min(group['time']),
        'latest_review_date': max(group['time'])
    }

# Write the JSON output to a file
with open('/content/task1_124.json', 'w', encoding='utf-8') as f:
  json.dump(json_output, f, indent=3)

print("task1_124.json file successfully generated!")

"""## 9. References:


*   [1] https://www.geeksforgeeks.org/how-to-read-excel-multiple-sheets-in-python-pandas/
*   [2] https://pynative.com/python-regex-flags/
*   [3] https://gist.github.com/Alex-Just/e86110836f3f93fe7932290526529cd1

[Link to my workspace](https://colab.research.google.com/drive/1bghOQ5NYKWxSBQdVG22qmqm98hHHSmS3?usp=sharing)
"""